<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>POSTERIOR AGREEMENT FOR 1D CONNECTIVITY</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-11-07"><meta name="DC.source" content="OU_simulation2.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>POSTERIOR AGREEMENT FOR 1D CONNECTIVITY</h1><!--introduction--><p><b>Author: Nico Stephan Gorbach</b> , Institute of Machine Learning, ETHZ, email: <a href="mailto:nico.gorbach@inf.ethz.ch">nico.gorbach@inf.ethz.ch</a></p><p>Implementation of " <b>Pipeline Validation for Connectivity-based Cortex Parcellation</b> " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#3">Input</a></li><li><a href="#6">Generate noisy trajectories</a></li><li><a href="#7">Connectivity matrices</a></li><li><a href="#10">Deterministic annealing</a></li><li><a href="#12">Perturb centroids</a></li><li><a href="#13">Expectation maximization</a></li><li><a href="#14">Costs for histogram clustering given instance 1</a></li><li><a href="#15">Costs for histogram clustering given instance 2</a></li><li><a href="#16">Gibbs distribution 1</a></li><li><a href="#17">Gibbs distribution 2</a></li><li><a href="#18">Joint Gibbs distribution</a></li><li><a href="#19">Centroids for instance 1</a></li><li><a href="#20">Centroids for instance 2</a></li><li><a href="#22">Match clusters across data instances</a></li><li><a href="#23">Log partition sum for instance 1</a></li><li><a href="#24">Log partition sum for instance 2</a></li><li><a href="#25">Joint log partition sum</a></li><li><a href="#27">Generalization capacity</a></li><li><a href="#29">Number of equivariant transformations</a></li><li><a href="#32">Results</a></li><li><a href="#34">Discussion</a></li></ul></div><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>; clc
</pre><h2 id="2">Introduction</h2><p>This code documentation demonstrates that seed clusters become indistinguishable with increasing noise in the target connectivity scores. Ornstein-Uhlenbeck (OU) processes with different initial conditions were used to emulate fiber tracking between seed and target objects.Seed objects are grouped based upon their connectivity scores to target objects which were obtained by sampling from the OU-process <img src="OU_simulation2_eq12799452072342510472.png" alt="$200$" style="width:16px;height:8px;"> times. The means of the OU processes were selected such that, at low and medium SDE diffusion coefficients, three clusters of seed objects can be statistically distinguished from each other based soley on the connectivity scores of the seed objects. Contrastingly, at a high SDE diffusion coefficient the overlap in connectivity scores attributed to different seed objects become so great such that only two (and not three) clusters of seed objects can be statistically distinguished from each other.</p><h2 id="3">Input</h2><p>Set the number of potential clusters, the number of Ornstein-Uhlenbeck samples per seed coordinate and the diffusion term.</p><pre class="codeinput">K = 3;                          <span class="comment">% number of potential clusters</span>
ntrials = 1000;                 <span class="comment">% number of Ornstein-Uhlenbeck trials</span>
diffusion = [0.1,3,10];          <span class="comment">% diffusion in Ornstein-Uhlenbeck process</span>
final_time = 10;                <span class="comment">% final time in Ornstein-Uhlenbeck process</span>
seed_coords = 1:60;              <span class="comment">% seed coordinates</span>
target_coord = [10,12,20,50];   <span class="comment">% target coordinates</span>
</pre><pre class="codeinput"><span class="keyword">for</span> n = 1:length(diffusion)
</pre><h2 id="6">Generate noisy trajectories</h2><p>Connectivity matrix is generated by sampling from the Ornstein-Uhlenbeck process</p><pre class="codeinput">    target_coord_assign(:,1) = [ones(20,1);3*ones(20,1),;4*ones(20,1)];
    target_coord_assign(:,2) = [2*ones(40,1);4*ones(20,1)];
    [~,~,true_cluster_labels(:,n)] = unique(target_coord_assign,<span class="string">'rows'</span>);
    nSamples = 10;
    edges = 1:1:80;

    <span class="keyword">for</span> l = 1:ntrials
        <span class="keyword">for</span> i = 1:length(seed_coords)
            <span class="keyword">for</span> k = 1:size(target_coord_assign,2)
                [truth(l).trajectory(i,:,k),time] = OU_process(seed_coords(i),target_coord(target_coord_assign(i,k)),final_time,diffusion(n));
            <span class="keyword">end</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% packing</span>
    trajectory_all{n} = [reshape(target_coord_assign,[],1),[truth(1).trajectory(:,:,1);truth(1).trajectory(:,:,2)]];
</pre><h2 id="7">Connectivity matrices</h2><pre class="codeinput">    <span class="keyword">for</span> m = 1:2
        connectivity_matrix{n}{m} = [];
        <span class="keyword">for</span> i = 1:length(seed_coords)
            clear <span class="string">trajectory</span>;
            <span class="keyword">for</span> j = 1:nSamples
                k =randi(2);
                [trajectory(j,:),time] = OU_process(seed_coords(i),target_coord(target_coord_assign(i,k)),final_time,diffusion(n));
            <span class="keyword">end</span>
            connectivity_matrix{n}{m}(i,:) = histcounts(trajectory(:,end),edges);
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% remove columns of zeros</span>
    rem_idx(n,:) = sum(connectivity_matrix{n}{1},1) + sum(connectivity_matrix{n}{2},1) == 0;
    connectivity_matrix{n}{1}(:,rem_idx(n,:)) = [];
    connectivity_matrix{n}{2}(:,rem_idx(n,:)) = [];
</pre><pre class="codeinput">    <span class="comment">% normalize connectivity matrix</span>

    data1 = connectivity_matrix{n}{1};
    data2 = connectivity_matrix{n}{2};
</pre><pre class="codeinput">    <span class="comment">% start timer</span>
    tic;
</pre><h2 id="10">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput">    <span class="comment">% Annealing settings</span>
    beta_init = 0.1;                                 <span class="comment">% starting inverse temperature</span>
    beta_step = 1.02;                                <span class="comment">% inverse temperature step</span>
    beta_stop = 5;                                  <span class="comment">% stopping inverse temperature</span>
    perturb_sd = 0.01;                                <span class="comment">% centroid perturbation</span>

    <span class="comment">% Initialization of Gibbs distributions</span>
    gibbs_dist1 = ones(size(data1,1),K) ./ K;
    gibbs_dist2 = ones(size(data2,1),K) ./ K;

    <span class="comment">% Initialization of centroids</span>
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
    centroid1(centroid1==0) = eps;
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
    centroid2(centroid2==0) = eps;

    j = 0; beta = beta_init;
    <span class="keyword">while</span> beta &lt; beta_stop
</pre><h2 id="12">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="OU_simulation2_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">        <span class="comment">%r = perturb_sd * rand(size(centroid1));</span>
        centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); <span class="comment">% normalize</span>
        centroid2 = centroid2 + perturb_sd * rand(size(centroid1));
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); <span class="comment">% normalize</span>
</pre><h2 id="13">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">        <span class="keyword">for</span> iter = 1:80
</pre><h2 id="14">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="OU_simulation2_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential1 = -data1 * log(centroid1)';
</pre><h2 id="15">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="OU_simulation2_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential2 = -data2 * log(centroid2)';
</pre><h2 id="16">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="OU_simulation2_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum1==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),K);
                gibbs_dist1(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="17">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="OU_simulation2_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum2==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),K);
                gibbs_dist2(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="18">Joint Gibbs distribution</h2><p>Maximum entropy distribution: <img src="OU_simulation2_eq01026291022421135752.png" alt="$p_{ik}^{(1,2)} = \exp \left(-\beta ( R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$" style="width:159px;height:20px;"></p><pre class="codeinput">            dist_joint = exp(-beta * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
</pre><h2 id="19">Centroids for instance 1</h2><p>Probability prototype: <img src="OU_simulation2_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
            centroid1(centroid1==0) = eps;
</pre><h2 id="20">Centroids for instance 2</h2><p>Probability prototype: <img src="OU_simulation2_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
            centroid2(centroid2==0) = eps;
        <span class="keyword">end</span>
</pre><pre class="codeinput">        beta = beta * beta_step;    <span class="comment">% increase inverse temperature (complexity)</span>
</pre><h2 id="22">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">        <span class="comment">%if beta &lt; 10</span>
        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
        <span class="comment">%end</span>
</pre><h2 id="23">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="OU_simulation2_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost1 = -beta * potential1;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="24">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="OU_simulation2_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost2 = -beta * potential2;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="25">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="OU_simulation2_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">        joint_scaled_cost = -beta * (potential1 + potential2);
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
</pre><pre class="codeinput">        j = j+1;
</pre><h2 id="27">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="OU_simulation2_eq05853161460555252620.png" alt="$GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:221px;height:14px;"></p><pre class="codeinput">        gc{n}(j) = log(K) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
            - log_partition_sum2) ./ size(partition_sum1,1);

        <span class="comment">% pack</span>
        inv_temp(j) = beta;
        gibbs_dist_packed1{n}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2{n}(:,:,j) = gibbs_dist2;
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="29">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="OU_simulation2_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">    d = sum(round(gibbs_dist1),1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';

    <span class="comment">% correct generalization capacity</span>
    gc{n} = gc{n} - log(K) + nTransformations;
</pre><pre class="codeinput">    gc{n} = gc{n} * log2(exp(1));       <span class="comment">% transforming units from nats to bits</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><h2 id="32">Results</h2><pre class="codeinput">display_result(gc,inv_temp,gibbs_dist_packed1,gibbs_dist_packed2,diffusion,<span class="keyword">...</span>
    seed_coords,data1,trajectory_all,time,connectivity_matrix,rem_idx)
</pre><img vspace="5" hspace="5" src="OU_simulation2_01.png" style="width:1200px;height:600px;" alt=""> <img vspace="5" hspace="5" src="OU_simulation2_02.png" style="width:1000px;height:400px;" alt=""> <img vspace="5" hspace="5" src="OU_simulation2_03.png" style="width:1000px;height:400px;" alt=""> <img vspace="5" hspace="5" src="OU_simulation2_04.png" style="width:1200px;height:800px;" alt=""> <pre class="codeinput">disp([<span class="string">'runtime: '</span> num2str(toc/60) <span class="string">' minutes'</span>]);     <span class="comment">% runtime</span>
</pre><pre class="codeoutput">runtime: 0.16734 minutes
</pre><h2 id="34">Discussion</h2><p>The generalization capacity ranks the experiments in the order corresponding to the stochasticity (i.e. magnitude of SDE diffusion coefficient) in the Ornstein-Uhlenbeck process which is expected since a stochasticity in the OU process results in more ``noisy'' connectivity scores. The ranking is further supported by the disagreement among the ERM's with red indicating disagreement of clusterings. The GCM's of the experiments characterized by the low and medium stochasticity resolve three effective clusters of seed objects based upon their connectivity scores to target objects. The experiment with stochasticity, however, sufficiently corrupts the connectivity scores such that clusters <img src="OU_simulation2_eq17262503285923366262.png" alt="$2$" style="width:5px;height:8px;"> and <img src="OU_simulation2_eq14956984979706529373.png" alt="$3$" style="width:5px;height:8px;"> become indistinguishable as indicated by the GCM.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% POSTERIOR AGREEMENT FOR 1D CONNECTIVITY
% *Author: Nico Stephan Gorbach* ,
% Institute of Machine Learning, ETHZ,
% email: nico.gorbach@inf.ethz.ch
%
% Implementation of " *Pipeline Validation for Connectivity-based Cortex Parcellation* 
% " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann 
%

%%

clear all; close all; clc

%% Introduction
% This code documentation demonstrates that seed clusters become indistinguishable 
% with increasing noise in the target connectivity scores. Ornstein-Uhlenbeck (OU) 
% processes with different initial conditions were used to emulate fiber tracking 
% between seed and target objects.Seed objects are grouped based upon their 
% connectivity scores to target objects which were obtained by sampling from 
% the OU-process $200$ times. The means of the OU processes were selected such that, 
% at low and medium SDE diffusion coefficients, three clusters of seed objects can 
% be statistically distinguished from each other based soley on the connectivity 
% scores of the seed objects. Contrastingly, at a high SDE diffusion coefficient 
% the overlap in connectivity scores attributed to different seed objects become 
% so great such that only two (and not three) clusters of seed objects can be 
% statistically distinguished from each other.

%% Input
% Set the number of potential clusters, the number of Ornstein-Uhlenbeck samples 
% per seed coordinate and the diffusion term.

K = 3;                          % number of potential clusters
ntrials = 1000;                 % number of Ornstein-Uhlenbeck trials
diffusion = [0.1,3,10];          % diffusion in Ornstein-Uhlenbeck process
final_time = 10;                % final time in Ornstein-Uhlenbeck process
seed_coords = 1:60;              % seed coordinates
target_coord = [10,12,20,50];   % target coordinates

%%    
for n = 1:length(diffusion)
    
    %% Generate noisy trajectories
    % Connectivity matrix is generated by sampling from the Ornstein-Uhlenbeck
    % process
    
    target_coord_assign(:,1) = [ones(20,1);3*ones(20,1),;4*ones(20,1)];
    target_coord_assign(:,2) = [2*ones(40,1);4*ones(20,1)];
    [~,~,true_cluster_labels(:,n)] = unique(target_coord_assign,'rows');
    nSamples = 10;
    edges = 1:1:80;
    
    for l = 1:ntrials
        for i = 1:length(seed_coords)
            for k = 1:size(target_coord_assign,2)
                [truth(l).trajectory(i,:,k),time] = OU_process(seed_coords(i),target_coord(target_coord_assign(i,k)),final_time,diffusion(n));
            end
        end
    end
    
    % packing
    trajectory_all{n} = [reshape(target_coord_assign,[],1),[truth(1).trajectory(:,:,1);truth(1).trajectory(:,:,2)]];
    
    %% Connectivity matrices
    for m = 1:2
        connectivity_matrix{n}{m} = [];
        for i = 1:length(seed_coords)
            clear trajectory;
            for j = 1:nSamples
                k =randi(2);
                [trajectory(j,:),time] = OU_process(seed_coords(i),target_coord(target_coord_assign(i,k)),final_time,diffusion(n));
            end
            connectivity_matrix{n}{m}(i,:) = histcounts(trajectory(:,end),edges);
        end
    end
    
    % remove columns of zeros
    rem_idx(n,:) = sum(connectivity_matrix{n}{1},1) + sum(connectivity_matrix{n}{2},1) == 0;
    connectivity_matrix{n}{1}(:,rem_idx(n,:)) = [];
    connectivity_matrix{n}{2}(:,rem_idx(n,:)) = [];
    
    %%
    
    % normalize connectivity matrix
    
    data1 = connectivity_matrix{n}{1};
    data2 = connectivity_matrix{n}{2};
    
    %%
    
    % start timer
    tic;
    
    %% Deterministic annealing
    % Determine global minimizer.
    
    % Annealing settings
    beta_init = 0.1;                                 % starting inverse temperature
    beta_step = 1.02;                                % inverse temperature step
    beta_stop = 5;                                  % stopping inverse temperature
    perturb_sd = 0.01;                                % centroid perturbation
    
    % Initialization of Gibbs distributions
    gibbs_dist1 = ones(size(data1,1),K) ./ K;
    gibbs_dist2 = ones(size(data2,1),K) ./ K;
    
    % Initialization of centroids
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
    centroid1(centroid1==0) = eps;
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
    centroid2(centroid2==0) = eps;
    
    j = 0; beta = beta_init;
    while beta < beta_stop
        
        %%% Perturb centroids
        % Avoid local minimum by perturbing centroids:
        % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
        %r = perturb_sd * rand(size(centroid1));
        centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); % normalize
        centroid2 = centroid2 + perturb_sd * rand(size(centroid1));
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); % normalize
        
        %% Expectation maximization
        % Iterate between determining Gibbs distributions and maximzing
        % variational lower bound w.r.t. centroids.
        for iter = 1:80
            
            %%% Costs for histogram clustering given instance 1
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
            potential1 = -data1 * log(centroid1)';
            
            %%% Costs for histogram clustering given instance 2
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
            potential2 = -data2 * log(centroid2)';
            
            %%% Gibbs distribution 1
            % Maximum entropy distribution:
            % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
            
            % avoid underflow
            idx = find(partition_sum1==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),K);
                gibbs_dist1(max_ind) = 1;
            end
            
            %%% Gibbs distribution 2
            % Maximum entropy distribution:
            % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
            
            % avoid underflow
            idx = find(partition_sum2==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),K);
                gibbs_dist2(max_ind) = 1;
            end
            
            %%% Joint Gibbs distribution
            % Maximum entropy distribution:
            % $p_{ik}^{(1,2)} = \exp \left(-\beta (
            % R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$
            dist_joint = exp(-beta * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
            
            %%% Centroids for instance 1
            % Probability prototype:
            % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
            centroid1(centroid1==0) = eps;
            
            %%% Centroids for instance 2
            % Probability prototype:
            % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
            centroid2(centroid2==0) = eps;
        end
        
        %%
        
        beta = beta * beta_step;    % increase inverse temperature (complexity)
        
        %%% Match clusters across data instances
        % Use Hungarian algorithm to match clusters.
        %if beta < 10
        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
        %end
        %%% Log partition sum for instance 1
        % Determine log partition sum while avoiding underflow:
        % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
        scaled_cost1 = -beta * potential1;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
        
        %%% Log partition sum for instance 2
        % Determine log partition sum while avoiding underflow:
        % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
        scaled_cost2 = -beta * potential2;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
        
        %%% Joint log partition sum
        % Determine joint log partition sum while avoiding underflow:
        % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
        joint_scaled_cost = -beta * (potential1 + potential2);
        % log-sum-exp trick to prevent underflow
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
        
        %%
        j = j+1;
        
        %%% Generalization capacity
        % Resolution of the hypothesis space:
        % $GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
        gc{n}(j) = log(K) + sum(log_joint_partition_sum - log_partition_sum1 ...
            - log_partition_sum2) ./ size(partition_sum1,1);
        
        % pack
        inv_temp(j) = beta;
        gibbs_dist_packed1{n}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2{n}(:,:,j) = gibbs_dist2;
        
    end
    
    %% Number of equivariant transformations
    % Richness of the hypothesis space:
    % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
    d = sum(round(gibbs_dist1),1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';
    
    % correct generalization capacity
    gc{n} = gc{n} - log(K) + nTransformations;
    
    %%
    
    gc{n} = gc{n} * log2(exp(1));       % transforming units from nats to bits
  
end

%% Results

display_result(gc,inv_temp,gibbs_dist_packed1,gibbs_dist_packed2,diffusion,...
    seed_coords,data1,trajectory_all,time,connectivity_matrix,rem_idx)
%%

disp(['runtime: ' num2str(toc/60) ' minutes']);     % runtime

%% Discussion
% The generalization capacity ranks the experiments in the order corresponding to 
% the stochasticity (i.e. magnitude of SDE diffusion coefficient) in the Ornstein-Uhlenbeck process which is expected 
% since a stochasticity in the OU process results in more ``noisy'' connectivity 
% scores. The ranking is further supported by the disagreement among the ERM's with 
% red indicating disagreement of clusterings. The GCM's of the experiments characterized 
% by the low and medium stochasticity resolve three effective clusters of seed objects 
% based upon their connectivity scores to target objects. The experiment with stochasticity, 
% however, sufficiently corrupts the connectivity scores such that clusters $2$ and $3$ 
% become indistinguishable as indicated by the GCM.
##### SOURCE END #####
--></body></html>


<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>POSTERIOR AGREEMENT FOR CLUSTERING IN THE PROBABILITY SIMPLEX</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-11-07"><meta name="DC.source" content="PA_for_clustering_in_prob_simplex2.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>POSTERIOR AGREEMENT FOR CLUSTERING IN THE PROBABILITY SIMPLEX</h1><!--introduction--><p><b>Author: Nico Stephan Gorbach</b> , Institute of Machine Learning, ETHZ, email: <a href="mailto:nico.gorbach@inf.ethz.ch">nico.gorbach@inf.ethz.ch</a></p><p>Implementation of " <b>Pipeline Validation for Connectivity-based Cortex Parcellation</b> " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann</p><p>Simulation for validation by posterior agreeement for clustering in the probability simplex.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#3">Input</a></li><li><a href="#4">Generate data from the probability simplex</a></li><li><a href="#7">Histogram clustering</a></li><li><a href="#8">Deterministic annealing</a></li><li><a href="#11">Perturb centroids</a></li><li><a href="#12">Expectation maximization</a></li><li><a href="#13">Costs for histogram clustering given instance 1</a></li><li><a href="#14">Costs for histogram clustering given instance 2</a></li><li><a href="#15">Gibbs distribution 1</a></li><li><a href="#16">Gibbs distribution 2</a></li><li><a href="#17">Joint Gibbs distribution</a></li><li><a href="#18">Centroids for instance 1</a></li><li><a href="#19">Centroids for instance 2</a></li><li><a href="#20">Match clusters across data instances</a></li><li><a href="#21">Log partition sum for instance 1</a></li><li><a href="#22">Log partition sum for instance 2</a></li><li><a href="#23">Joint log partition sum</a></li><li><a href="#24">Generalization capacity</a></li><li><a href="#26">Number of equivariant transformations</a></li><li><a href="#27">Information content</a></li><li><a href="#29">Bayesian Information Criterion (BIC)</a></li><li><a href="#30">Akaike Information Criterion (AIC)</a></li><li><a href="#32">Bayesian evidence</a></li><li><a href="#34">Kmeans</a></li><li><a href="#35">Deterministic annealing</a></li><li><a href="#38">Perturb centroids</a></li><li><a href="#39">Expectation maximization</a></li><li><a href="#40">Costs for histogram clustering given instance 1</a></li><li><a href="#41">Costs for histogram clustering given instance 2</a></li><li><a href="#42">Gibbs distribution 1</a></li><li><a href="#43">Gibbs distribution 2</a></li><li><a href="#44">Centroids for instance 1</a></li><li><a href="#45">Centroids for instance 2</a></li><li><a href="#46">Match clusters across data instances</a></li><li><a href="#47">Log partition sum for instance 1</a></li><li><a href="#48">Log partition sum for instance 2</a></li><li><a href="#49">Joint log partition sum</a></li><li><a href="#50">Generalization capacity</a></li><li><a href="#52">Number of equivariant transformations</a></li><li><a href="#53">Information content</a></li><li><a href="#55">Results</a></li><li><a href="#56">Discussion</a></li></ul></div><pre class="codeinput"><span class="comment">% clear workspace, close figures</span>
clear <span class="string">all</span>; close <span class="string">all</span>; clc;
</pre><h2 id="2">Introduction</h2><p>In this instructional code we demonstrate validation by posterior agreement for clustering in the probability simplex whereby the connectivity scores are sampled from the Dirichlet distribution. Additionally, we validate clustering across more degrees of freedom different potential clusters by determining the maximum generalization capacity for different potential clusters. We refer to the maximum generalization capacity as the information content of the algorithm.</p><h2 id="3">Input</h2><p>Set the number of objects to cluster, the number of true clusters and the number of potential clusters for estimation.</p><pre class="codeinput">n = 2500;      <span class="comment">% number of objects</span>
ktrue = 9;     <span class="comment">% true number of clusters</span>
K = 2:15;      <span class="comment">% number of potential clusters</span>
</pre><h2 id="4">Generate data from the probability simplex</h2><p>Sample data points from the Dirichlet distribution.</p><pre class="codeinput">dim = 3;       <span class="comment">% number of dimensions</span>

<span class="comment">% sample Dirichlet parameters</span>
<span class="keyword">for</span> i = 1:ktrue
    dirichlet_param(i,:) = dirichletRnd(1,ones(1,3)./3);
<span class="keyword">end</span>

<span class="comment">% sample probability measurements from Dirichlet distribution</span>
<span class="keyword">for</span> k = 1:size(dirichlet_param,1)
    <span class="keyword">for</span> i = 1:floor(n/size(dirichlet_param,1))
        cluster1{k}(i,:) = dirichletRnd(5e1,dirichlet_param(k,:));
        cluster2{k}(i,:) = dirichletRnd(5e1,dirichlet_param(k,:));
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% pack data</span>
data1 = [cluster1{:}]; data1 = reshape(data1',dim,[])';
data2 = [cluster2{:}]; data2 = reshape(data2',dim,[])';

<span class="comment">% sample very noisy data</span>
<span class="keyword">for</span> i = 1:500
    corruption1(i,:) = dirichletRnd(3e0,ones(1,3)./3);
    corruption2(i,:) = dirichletRnd(3e0,ones(1,3)./3);
<span class="keyword">end</span>

<span class="comment">% pack and scale data</span>
data1 = 500*[data1;corruption1];
data2 = 500*[data2;corruption2];

<span class="comment">%h = setup_plots;</span>
</pre><pre class="codeinput"><span class="comment">% preprocessing for Bayesian evidence</span>

alpha{1} = full(sum(data1,1));
alpha{2} = full(sum(data1,1))/100;
</pre><pre class="codeinput"><span class="comment">% start timer</span>
tic;
</pre><h2 id="7">Histogram clustering</h2><h2 id="8">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput"><span class="comment">% Annealing settings</span>
beta_init = 1e-3;     <span class="comment">% starting inverse temperature</span>
beta_step = 1.1;      <span class="comment">% inverse temperature step</span>
beta_stop = 1e0;      <span class="comment">% stopping inverse temperature</span>
perturb_sd = 1e-6;    <span class="comment">% centroid perturbation</span>

<span class="comment">% Initialization of information content</span>
info_content.hc = zeros(1,max(K));

<span class="keyword">for</span> k = K
</pre><pre class="codeinput">    <span class="comment">% Initialization of Gibbs distributions</span>
    gibbs_dist1 = ones(size(data1,1),k) ./ k;
    gibbs_dist2 = ones(size(data2,1),k) ./ k;

    <span class="comment">% Initialization of centroids</span>
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
    centroid1(centroid1==0) = eps;
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
    centroid2(centroid2==0) = eps;

    j = 0; beta = beta_init;
    <span class="keyword">while</span> beta &lt;= beta_stop
</pre><h2 id="11">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="PA_for_clustering_in_prob_simplex2_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">        centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); <span class="comment">% normalize</span>
        centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); <span class="comment">% normalize</span>
</pre><h2 id="12">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">        <span class="keyword">for</span> iter = 1:10
</pre><h2 id="13">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="PA_for_clustering_in_prob_simplex2_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential1 = -data1 * log(centroid1)';
</pre><h2 id="14">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="PA_for_clustering_in_prob_simplex2_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential2 = -data2 * log(centroid2)';
</pre><h2 id="15">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="PA_for_clustering_in_prob_simplex2_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum1==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);
                gibbs_dist1(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="16">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="PA_for_clustering_in_prob_simplex2_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum2==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);
                gibbs_dist2(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="17">Joint Gibbs distribution</h2><p>Maximum entropy distribution: <img src="PA_for_clustering_in_prob_simplex2_eq01026291022421135752.png" alt="$p_{ik}^{(1,2)} = \exp \left(-\beta ( R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$" style="width:159px;height:20px;"></p><pre class="codeinput">            dist_joint = exp(-beta * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
</pre><h2 id="18">Centroids for instance 1</h2><p>Probability prototype: <img src="PA_for_clustering_in_prob_simplex2_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
            centroid1(centroid1==0) = eps;
</pre><h2 id="19">Centroids for instance 2</h2><p>Probability prototype: <img src="PA_for_clustering_in_prob_simplex2_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
            centroid2(centroid2==0) = eps;

        <span class="keyword">end</span>

        <span class="comment">% increase inverse temperature:</span>
        beta = beta * beta_step;
</pre><h2 id="20">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
</pre><h2 id="21">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost1 = -beta * potential1;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="22">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost2 = -beta * potential2;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="23">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">        joint_scaled_cost = -beta * (potential1 + potential2);
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));

        j = j+1;
</pre><h2 id="24">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="PA_for_clustering_in_prob_simplex2_eq05853161460555252620.png" alt="$GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:221px;height:14px;"></p><pre class="codeinput">        gc.hc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
            - log_partition_sum2) ./ size(partition_sum1,1);

        <span class="comment">% store results</span>
        inv_temp.hc(j) = beta;
        gibbs_dist_packed1.hc{k}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2.hc{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);

        <span class="comment">% plot results</span>
        <span class="comment">%plot(h{2},inv_temp,gc{k},'LineWidth',2); drawnow % plot generalization capacity</span>
        <span class="comment">%simplex(h{1},data1(:,1),data1(:,2),data1(:,3),gibbs_dist1); % display Gibbs distribution on probability simplex</span>
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="26">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="PA_for_clustering_in_prob_simplex2_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';

     <span class="comment">% correct generalization capacity</span>
    gc.hc{k} = gc.hc{k}-log(k)+nTransformations;

    gc.hc{k} = gc.hc{k} * log2(exp(1));  <span class="comment">% transforming units from nats to bits</span>
</pre><h2 id="27">Information content</h2><p>Quality of algorithm: <img src="PA_for_clustering_in_prob_simplex2_eq12990683164727386852.png" alt="$\mathcal{I} = \max_{\beta} GC(\beta)$" style="width:81px;height:12px;"></p><pre class="codeinput">    [info_content.hc(k),max_gc_idx.hc(k)] = max(gc.hc{k});
</pre><pre class="codeinput">    <span class="comment">% check if emprical risk minimizer (ERM) contains k clusters</span>
    <span class="keyword">if</span> sum(logical(sum(gibbs_dist1,1)))==k
</pre><h2 id="29">Bayesian Information Criterion (BIC)</h2><p><img src="PA_for_clustering_in_prob_simplex2_eq08376778382015985202.png" alt="$BIC := m \times k \times ln (n) + 2 R(c^{\perp},\mathbf{X})$" style="width:170px;height:12px;"> where <img src="PA_for_clustering_in_prob_simplex2_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="PA_for_clustering_in_prob_simplex2_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">    cost(k) = sum(sum(gibbs_dist1 .* (-bsxfun(@rdivide,data1,sum(data1,2)) * log(centroid1)')));
    <span class="comment">%cost = sum(sum(gibbs_dist1 .* (-data1 * log(centroid1)')));</span>
    BIC(k) = size(data1,2) * k * log(size(data1,1)) + 2 * cost(k);
</pre><h2 id="30">Akaike Information Criterion (AIC)</h2><p><img src="PA_for_clustering_in_prob_simplex2_eq07844434431064143690.png" alt="$AIC := 2 \times m \times k + 2 R(c^{\perp},\mathbf{X})$" style="width:149px;height:12px;"> where <img src="PA_for_clustering_in_prob_simplex2_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="PA_for_clustering_in_prob_simplex2_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">    AIC(k) = 2 * size(data1,2) * k + 2 * cost(k);
</pre><pre class="codeinput">    <span class="keyword">else</span>
        BIC(k) = NaN;
        AIC(k) = NaN;
    <span class="keyword">end</span>
</pre><h2 id="32">Bayesian evidence</h2><p><img src="PA_for_clustering_in_prob_simplex2_eq08403349875303284569.png" alt="$p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi) p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k + \mathbf{n}_k)}{B(\alpha_k)}$" style="width:220px;height:18px;"> where <img src="PA_for_clustering_in_prob_simplex2_eq08059537060765087149.png" alt="$\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$" style="width:113px;height:11px;"> and <img src="PA_for_clustering_in_prob_simplex2_eq07727016875268873876.png" alt="$n_{kj} := \sum_{i:c(i)=k} x_{ij}$" style="width:86px;height:14px;"></p><pre class="codeinput">    [~,labels] = max(gibbs_dist1,[],2);
    log_bayes_evidence{1}(k) = 0;
    log_bayes_evidence{2}(k) = 0;
    <span class="keyword">for</span> c = unique(labels)'
        ncj = full(sum(data1(labels==c,:),1));
        alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
        alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
        ncj(ncj==0) = [];

        log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) <span class="keyword">...</span>
            + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
        log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) <span class="keyword">...</span>
            + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
    <span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><h2 id="34">Kmeans</h2><h2 id="35">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput"><span class="comment">% Annealing settings</span>
beta_init = 1e-6;     <span class="comment">% starting inverse temperature</span>
beta_step = 1.1;      <span class="comment">% inverse temperature step</span>
beta_stop = 5e-3;      <span class="comment">% stopping inverse temperature</span>
perturb_sd = 1e-6;    <span class="comment">% centroid perturbation</span>

<span class="comment">% Initialization of information content</span>
info_content.kmeans = zeros(1,max(K));

<span class="keyword">for</span> k = K
</pre><pre class="codeinput">    <span class="comment">% Initialization of Gibbs distributions</span>
    gibbs_dist1 = ones(size(data1,1),k) ./ k;
    gibbs_dist2 = ones(size(data2,1),k) ./ k;

    <span class="comment">% Initialization of centroids</span>
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(gibbs_dist1,1)');
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(gibbs_dist2,1)');

    j = 0; beta = beta_init;
    <span class="keyword">while</span> beta &lt;= beta_stop
</pre><h2 id="38">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="PA_for_clustering_in_prob_simplex2_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">        centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
        centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
</pre><h2 id="39">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">        <span class="keyword">for</span> iter = 1:10
</pre><h2 id="40">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="PA_for_clustering_in_prob_simplex2_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential1 = pdist2(data1,centroid1,<span class="string">'squaredeuclidean'</span>);
</pre><h2 id="41">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="PA_for_clustering_in_prob_simplex2_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential2 = pdist2(data2,centroid2,<span class="string">'squaredeuclidean'</span>);
</pre><h2 id="42">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="PA_for_clustering_in_prob_simplex2_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum1==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);
                gibbs_dist1(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="43">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="PA_for_clustering_in_prob_simplex2_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum2==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);
                gibbs_dist2(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="44">Centroids for instance 1</h2><p>Probability prototype: <img src="PA_for_clustering_in_prob_simplex2_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(gibbs_dist1,1)');
</pre><h2 id="45">Centroids for instance 2</h2><p>Probability prototype: <img src="PA_for_clustering_in_prob_simplex2_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(gibbs_dist2,1)');

        <span class="keyword">end</span>

        <span class="comment">% increase inverse temperature:</span>
        beta = beta * beta_step;
</pre><h2 id="46">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
</pre><h2 id="47">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost1 = -beta * potential1;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="48">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost2 = -beta * potential2;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="49">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="PA_for_clustering_in_prob_simplex2_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">        joint_scaled_cost = -beta * (potential1 + potential2);
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));

        j = j+1;
</pre><h2 id="50">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="PA_for_clustering_in_prob_simplex2_eq05853161460555252620.png" alt="$GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:221px;height:14px;"></p><pre class="codeinput">        gc.kmeans{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
            - log_partition_sum2) ./ size(partition_sum1,1);

        <span class="comment">% store results</span>
        inv_temp.kmeans(j) = beta;
        gibbs_dist_packed1.kmeans{k}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2.kmeans{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);

        <span class="comment">% plot results</span>
        <span class="comment">%plot(h{2},inv_temp,gc{k},'LineWidth',2); drawnow % plot generalization capacity</span>
        <span class="comment">%simplex(h{1},data1(:,1),data1(:,2),data1(:,3),gibbs_dist1); % display Gibbs distribution on probability simplex</span>
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="52">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="PA_for_clustering_in_prob_simplex2_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';

     <span class="comment">% correct generalization capacity</span>
    gc.kmeans{k} = gc.kmeans{k}-log(k)+nTransformations;

    gc.kmeans{k} = gc.kmeans{k} * log2(exp(1));  <span class="comment">% transforming units from nats to bits</span>
</pre><h2 id="53">Information content</h2><p>Quality of algorithm: <img src="PA_for_clustering_in_prob_simplex2_eq12990683164727386852.png" alt="$\mathcal{I} = \max_{\beta} GC(\beta)$" style="width:81px;height:12px;"></p><pre class="codeinput">    [info_content.kmeans(k),max_gc_idx.kmeans(k)] = max(gc.kmeans{k});
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><h2 id="55">Results</h2><pre class="codeinput">display_result(info_content,gc,max_gc_idx,inv_temp,BIC,AIC,log_bayes_evidence,<span class="keyword">...</span>
    gibbs_dist_packed1,gibbs_dist_packed2,data1,k,K)
</pre><pre class="codeoutput">runtime = 0.81384 minutes
</pre><img vspace="5" hspace="5" src="PA_for_clustering_in_prob_simplex2_01.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="PA_for_clustering_in_prob_simplex2_02.png" style="width:1600px;height:300px;" alt=""> <img vspace="5" hspace="5" src="PA_for_clustering_in_prob_simplex2_03.png" style="width:560px;height:420px;" alt=""> <h2 id="56">Discussion</h2><p>The posterior agreement is compared to alternative validation criteria including BIC, AIC and Bayesian evidence We additionally determine Bayesian evidence for two different hyperparameter settings in order to investigate the sensitivity of ranking by Bayesian evidence on the hyperparameters.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% POSTERIOR AGREEMENT FOR CLUSTERING IN THE PROBABILITY SIMPLEX
% *Author: Nico Stephan Gorbach* ,
% Institute of Machine Learning, ETHZ,
% email: nico.gorbach@inf.ethz.ch
%
% Implementation of " *Pipeline Validation for Connectivity-based Cortex Parcellation* 
% " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann 
%
% Simulation for validation by posterior agreeement for clustering in the 
% probability simplex.
%

%%

% clear workspace, close figures
clear all; close all; clc;

%% Introduction
% In this instructional code we demonstrate validation by posterior agreement 
% for clustering in the probability simplex whereby the connectivity scores 
% are sampled from the Dirichlet distribution. Additionally, we validate clustering 
% across more degrees of freedom different potential clusters by determining the 
% maximum generalization capacity for different potential clusters. We refer to the 
% maximum generalization capacity as the information content of the algorithm.

%% Input
% Set the number of objects to cluster, the number of true clusters and the
% number of potential clusters for estimation.

n = 2500;      % number of objects                                                              
ktrue = 9;     % true number of clusters
K = 2:15;      % number of potential clusters   

%% Generate data from the probability simplex
% Sample data points from the Dirichlet distribution.

dim = 3;       % number of dimensions

% sample Dirichlet parameters
for i = 1:ktrue
    dirichlet_param(i,:) = dirichletRnd(1,ones(1,3)./3);
end

% sample probability measurements from Dirichlet distribution
for k = 1:size(dirichlet_param,1)
    for i = 1:floor(n/size(dirichlet_param,1))
        cluster1{k}(i,:) = dirichletRnd(5e1,dirichlet_param(k,:));
        cluster2{k}(i,:) = dirichletRnd(5e1,dirichlet_param(k,:));
    end
end

% pack data
data1 = [cluster1{:}]; data1 = reshape(data1',dim,[])';
data2 = [cluster2{:}]; data2 = reshape(data2',dim,[])';

% sample very noisy data
for i = 1:500
    corruption1(i,:) = dirichletRnd(3e0,ones(1,3)./3);
    corruption2(i,:) = dirichletRnd(3e0,ones(1,3)./3);
end

% pack and scale data
data1 = 500*[data1;corruption1];
data2 = 500*[data2;corruption2];

%h = setup_plots;

%%

% preprocessing for Bayesian evidence

alpha{1} = full(sum(data1,1));
alpha{2} = full(sum(data1,1))/100;

%%

% start timer
tic;

%% Histogram clustering

%%% Deterministic annealing
% Determine global minimizer.

% Annealing settings
beta_init = 1e-3;     % starting inverse temperature
beta_step = 1.1;      % inverse temperature step
beta_stop = 1e0;      % stopping inverse temperature
perturb_sd = 1e-6;    % centroid perturbation

% Initialization of information content
info_content.hc = zeros(1,max(K));

for k = K
    % Initialization of Gibbs distributions
    gibbs_dist1 = ones(size(data1,1),k) ./ k;
    gibbs_dist2 = ones(size(data2,1),k) ./ k;
    
    % Initialization of centroids
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); 
    centroid1(centroid1==0) = eps;
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); 
    centroid2(centroid2==0) = eps;
    
    j = 0; beta = beta_init;
    while beta <= beta_stop
        
        %%% Perturb centroids
        % Avoid local minimum by perturbing centroids:
        % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
        centroid1 = centroid1 + perturb_sd * rand(size(centroid1)); 
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); % normalize
        centroid2 = centroid2 + perturb_sd * rand(size(centroid2)); 
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); % normalize

        %% Expectation maximization
        % Iterate between determining Gibbs distributions and maximzing
        % variational lower bound w.r.t. centroids.
        for iter = 1:10
         
            %%% Costs for histogram clustering given instance 1
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
            potential1 = -data1 * log(centroid1)';
            
            %%% Costs for histogram clustering given instance 2
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
            potential2 = -data2 * log(centroid2)'; 
           
            %%% Gibbs distribution 1
            % Maximum entropy distribution:
            % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
            
            % avoid underflow
            idx = find(partition_sum1==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);  
                gibbs_dist1(max_ind) = 1;
            end

            %%% Gibbs distribution 2
            % Maximum entropy distribution:
            % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
            
            % avoid underflow
            idx = find(partition_sum2==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);  
                gibbs_dist2(max_ind) = 1;
            end

            %%% Joint Gibbs distribution
            % Maximum entropy distribution:
            % $p_{ik}^{(1,2)} = \exp \left(-\beta (
            % R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$
            dist_joint = exp(-beta * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
            
            %%% Centroids for instance 1
            % Probability prototype:
            % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); 
            centroid1(centroid1==0) = eps;
            
            %%% Centroids for instance 2
            % Probability prototype:
            % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); 
            centroid2(centroid2==0) = eps;
            
        end
        
        % increase inverse temperature:
        beta = beta * beta_step;

        %%% Match clusters across data instances
        % Use Hungarian algorithm to match clusters.
        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
        
        %%% Log partition sum for instance 1
        % Determine log partition sum while avoiding underflow:
        % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
        scaled_cost1 = -beta * potential1;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
        
        %%% Log partition sum for instance 2
         % Determine log partition sum while avoiding underflow:
        % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
        scaled_cost2 = -beta * potential2;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
        
        %%% Joint log partition sum
        % Determine joint log partition sum while avoiding underflow:
        % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
        joint_scaled_cost = -beta * (potential1 + potential2);
        % log-sum-exp trick to prevent underflow
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
       
        j = j+1;
        
        %%% Generalization capacity
        % Resolution of the hypothesis space:
        % $GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
        gc.hc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 ...
            - log_partition_sum2) ./ size(partition_sum1,1);
        
        % store results
        inv_temp.hc(j) = beta;
        gibbs_dist_packed1.hc{k}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2.hc{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);
        
        % plot results
        %plot(h{2},inv_temp,gc{k},'LineWidth',2); drawnow % plot generalization capacity
        %simplex(h{1},data1(:,1),data1(:,2),data1(:,3),gibbs_dist1); % display Gibbs distribution on probability simplex
        
    end
    
    %% Number of equivariant transformations
    % Richness of the hypothesis space: 
    % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';
    
     % correct generalization capacity
    gc.hc{k} = gc.hc{k}-log(k)+nTransformations;
    
    gc.hc{k} = gc.hc{k} * log2(exp(1));  % transforming units from nats to bits
    
    %% Information content
    % Quality of algorithm: 
    % $\mathcal{I} = \max_{\beta} GC(\beta)$
    [info_content.hc(k),max_gc_idx.hc(k)] = max(gc.hc{k}); 
    
    %%
    
    % check if emprical risk minimizer (ERM) contains k clusters
    if sum(logical(sum(gibbs_dist1,1)))==k
    %% Bayesian Information Criterion (BIC)
    % $BIC := m \times k \times ln (n) + 2 R(c^{\perp},\mathbf{X})$ where $m$ is the 
    % number of bins and $c^{\perp}$ is the empirical risk minimizer
    
    cost(k) = sum(sum(gibbs_dist1 .* (-bsxfun(@rdivide,data1,sum(data1,2)) * log(centroid1)')));
    %cost = sum(sum(gibbs_dist1 .* (-data1 * log(centroid1)')));
    BIC(k) = size(data1,2) * k * log(size(data1,1)) + 2 * cost(k);
    
    %% Akaike Information Criterion (AIC)
    % $AIC := 2 \times m \times k + 2 R(c^{\perp},\mathbf{X})$ where $m$ is the 
    % number of bins and $c^{\perp}$ is the empirical risk minimizer
    
    AIC(k) = 2 * size(data1,2) * k + 2 * cost(k);
    
    else
        BIC(k) = NaN;
        AIC(k) = NaN;
    end
    %% Bayesian evidence
    % $p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi)
    % p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k +
    % \mathbf{n}_k)}{B(\alpha_k)}$ where $\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$ 
    % and $n_{kj} := \sum_{i:c(i)=k} x_{ij}$
    
    [~,labels] = max(gibbs_dist1,[],2);
    log_bayes_evidence{1}(k) = 0;
    log_bayes_evidence{2}(k) = 0;
    for c = unique(labels)'
        ncj = full(sum(data1(labels==c,:),1));
        alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
        alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
        ncj(ncj==0) = [];
        
        log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) ...
            + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
        log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) ...
            + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
    end
    
end


%% Kmeans

%%% Deterministic annealing
% Determine global minimizer.

% Annealing settings
beta_init = 1e-6;     % starting inverse temperature
beta_step = 1.1;      % inverse temperature step
beta_stop = 5e-3;      % stopping inverse temperature
perturb_sd = 1e-6;    % centroid perturbation

% Initialization of information content
info_content.kmeans = zeros(1,max(K));

for k = K
    % Initialization of Gibbs distributions
    gibbs_dist1 = ones(size(data1,1),k) ./ k;
    gibbs_dist2 = ones(size(data2,1),k) ./ k;
    
    % Initialization of centroids
    centroid1 = gibbs_dist1'*data1;
    centroid1 = bsxfun(@rdivide,centroid1,sum(gibbs_dist1,1)');
    centroid2 = gibbs_dist2'*data2;
    centroid2 = bsxfun(@rdivide,centroid2,sum(gibbs_dist2,1)');
    
    j = 0; beta = beta_init;
    while beta <= beta_stop
        
        %%% Perturb centroids
        % Avoid local minimum by perturbing centroids:
        % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
        centroid1 = centroid1 + perturb_sd * rand(size(centroid1)); 
        centroid2 = centroid2 + perturb_sd * rand(size(centroid2)); 

        %% Expectation maximization
        % Iterate between determining Gibbs distributions and maximzing
        % variational lower bound w.r.t. centroids.
        for iter = 1:10
         
            %%% Costs for histogram clustering given instance 1
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
            potential1 = pdist2(data1,centroid1,'squaredeuclidean');
            
            %%% Costs for histogram clustering given instance 2
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
            potential2 = pdist2(data2,centroid2,'squaredeuclidean'); 
           
            %%% Gibbs distribution 1
            % Maximum entropy distribution:
            % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
            gibbs_dist1 = exp(-beta * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
            
            % avoid underflow
            idx = find(partition_sum1==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);  
                gibbs_dist1(max_ind) = 1;
            end

            %%% Gibbs distribution 2
            % Maximum entropy distribution:
            % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
            gibbs_dist2 = exp(-beta * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
            
            % avoid underflow
            idx = find(partition_sum2==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);  
                gibbs_dist2(max_ind) = 1;
            end


            %%% Centroids for instance 1
            % Probability prototype:
            % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
            centroid1 = gibbs_dist1'*data1;
            centroid1 = bsxfun(@rdivide,centroid1,sum(gibbs_dist1,1)'); 
            
            %%% Centroids for instance 2
            % Probability prototype:
            % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
            centroid2 = gibbs_dist2'*data2;
            centroid2 = bsxfun(@rdivide,centroid2,sum(gibbs_dist2,1)'); 
            
        end
        
        % increase inverse temperature:
        beta = beta * beta_step;

        %%% Match clusters across data instances
        % Use Hungarian algorithm to match clusters.
        match_clusters_idx = munkres(pdist2(centroid1,centroid2));
        potential2=potential2(:,match_clusters_idx);
        
        %%% Log partition sum for instance 1
        % Determine log partition sum while avoiding underflow:
        % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
        scaled_cost1 = -beta * potential1;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
        
        %%% Log partition sum for instance 2
         % Determine log partition sum while avoiding underflow:
        % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
        scaled_cost2 = -beta * potential2;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
        
        %%% Joint log partition sum
        % Determine joint log partition sum while avoiding underflow:
        % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
        joint_scaled_cost = -beta * (potential1 + potential2);
        % log-sum-exp trick to prevent underflow
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
       
        j = j+1;
        
        %%% Generalization capacity
        % Resolution of the hypothesis space:
        % $GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
        gc.kmeans{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 ...
            - log_partition_sum2) ./ size(partition_sum1,1);
        
        % store results
        inv_temp.kmeans(j) = beta;
        gibbs_dist_packed1.kmeans{k}(:,:,j) = gibbs_dist1;
        gibbs_dist_packed2.kmeans{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);
        
        % plot results
        %plot(h{2},inv_temp,gc{k},'LineWidth',2); drawnow % plot generalization capacity
        %simplex(h{1},data1(:,1),data1(:,2),data1(:,3),gibbs_dist1); % display Gibbs distribution on probability simplex
        
    end
    
    %% Number of equivariant transformations
    % Richness of the hypothesis space: 
    % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';
    
     % correct generalization capacity
    gc.kmeans{k} = gc.kmeans{k}-log(k)+nTransformations;
    
    gc.kmeans{k} = gc.kmeans{k} * log2(exp(1));  % transforming units from nats to bits
    
    %% Information content
    % Quality of algorithm: 
    % $\mathcal{I} = \max_{\beta} GC(\beta)$
    [info_content.kmeans(k),max_gc_idx.kmeans(k)] = max(gc.kmeans{k}); 
    
end


%% Results

display_result(info_content,gc,max_gc_idx,inv_temp,BIC,AIC,log_bayes_evidence,...
    gibbs_dist_packed1,gibbs_dist_packed2,data1,k,K)

%% Discussion
% The posterior agreement is compared to alternative validation criteria including BIC, AIC and Bayesian evidence
% We additionally determine Bayesian evidence for two different hyperparameter settings in order to investigate the 
% sensitivity of ranking by Bayesian evidence on the hyperparameters.

##### SOURCE END #####
--></body></html>



<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (Low Fiber Spread)</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-11-08"><meta name="DC.source" content="context_connectivity_low_spread.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (Low Fiber Spread)</h1><!--introduction--><p><b>Author: Nico Stephan Gorbach</b> , Institute of Machine Learning, ETHZ, email: <a href="mailto:nico.gorbach@inf.ethz.ch">nico.gorbach@inf.ethz.ch</a></p><p>Implementation of " <b>Pipeline Validation for Connectivity-based Cortex Parcellation</b> " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann</p><p>Simulation for validation by posterior agreeement for clustering in an anatomical context</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#3">Input</a></li><li><a href="#5">Learn spline control points</a></li><li><a href="#10">Connectivity matrices</a></li><li><a href="#12">Deterministic annealing</a></li><li><a href="#15">Perturb centroids</a></li><li><a href="#16">Expectation maximization</a></li><li><a href="#17">Costs for histogram clustering given instance 1</a></li><li><a href="#18">Costs for histogram clustering given instance 2</a></li><li><a href="#19">Gibbs distribution 1</a></li><li><a href="#20">Gibbs distribution 2</a></li><li><a href="#21">Joint Gibbs distribution</a></li><li><a href="#22">Centroids for instance 1</a></li><li><a href="#23">Centroids for instance 2</a></li><li><a href="#25">Match clusters across data instances</a></li><li><a href="#26">Log partition sum for instance 1</a></li><li><a href="#27">Log partition sum for instance 2</a></li><li><a href="#28">Joint log partition sum</a></li><li><a href="#30">Generalization capacity</a></li><li><a href="#33">Number of equivariant transformations</a></li><li><a href="#34">Information content</a></li><li><a href="#35">Bayesian Information Criterion (BIC)</a></li><li><a href="#36">Akaike Information Criterion (AIC)</a></li><li><a href="#37">Bayesian evidence</a></li><li><a href="#39">Results</a></li><li><a href="#40">Update confusion matrices</a></li><li><a href="#42">Confusion Matrix</a></li></ul></div><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>; clc
</pre><h2 id="2">Introduction</h2><p>This instructional code validates clustering in an anatomical context by simulating connectivity scores from a seed region in the postcentral gyrus. Given a template connectivity structure we can describe the fibers underlying that connectivity structure by B&eacute;zier curves. B&eacute;zier curves are defined by a set of control points <img src="context_connectivity_low_spread_eq04341845410467366200.png" alt="$\mathbf{P}$" style="width:8px;height:8px;">:</p><p><img src="context_connectivity_low_spread_eq10989988532119641984.png" alt="$$ \mathbf{B} = \mathbf{A}(\mathbf{t}) ~ \mathbf{P} + \epsilon,&#xA;\qquad \epsilon \sim \mathcal{N}(\mathbf{0},\gamma \mathbf{I} ), $$" style="width:167px;height:12px;"></p><p>where <img src="context_connectivity_low_spread_eq11141810138104989172.png" alt="$\mathbf{t}$" style="width:5px;height:8px;"> is a vector of linearly spaced scalar values ranging from <img src="context_connectivity_low_spread_eq00202142981986870057.png" alt="$0$" style="width:5px;height:8px;"> to <img src="context_connectivity_low_spread_eq18395870634560867587.png" alt="$1$" style="width:4px;height:8px;"> and <img src="context_connectivity_low_spread_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;"> determines the spread of the fiber samples. For a cubic B&eacute;zier curve, matrix <img src="context_connectivity_low_spread_eq04088523938794390934.png" alt="$\mathbf{A}$" style="width:9px;height:8px;"> is given by:</p><p><img src="context_connectivity_low_spread_eq07893260694569411196.png" alt="$$ \mathbf{A}(\mathbf{t}) = (\mathbf{1} - \mathbf{t})^3, 3 (\mathbf{1} - \mathbf{t})^2 \mathbf{t}, 3(\mathbf{1}-\mathbf{t})\mathbf{t}^2, \mathbf{t}^3 ) $$" style="width:199px;height:13px;">.</p><p>Since B&eacute;zier curves are linear in the control points <img src="context_connectivity_low_spread_eq04341845410467366200.png" alt="$\mathbf{P}$" style="width:8px;height:8px;">, we can determine the control points by multiplying the left hand-side of equation \ref{eqn:bezier_curve} by the pseudo-inverse <img src="context_connectivity_low_spread_eq04088523938794390934.png" alt="$\mathbf{A}$" style="width:9px;height:8px;"> (i.e. <img src="context_connectivity_low_spread_eq05482758431359188095.png" alt="$\mathbf{P} = \mathbf{A}^+ ( \mathbf{B} - \epsilon )$" style="width:74px;height:12px;">). Notice that the first and last control points <img src="context_connectivity_low_spread_eq16182667400455286292.png" alt="$\mathbf{p}_1,\mathbf{p}_4 \in \mathbf{P}$" style="width:49px;height:10px;"> mark the start and end of the B&eacute;zier curve, respectively. We can therefore determine the connectivity scores by sampling from the marginal distribution of <img src="context_connectivity_low_spread_eq12997305262010432456.png" alt="$\mathbf{p}_4$" style="width:11px;height:8px;">:</p><p><img src="context_connectivity_low_spread_eq03423795116937024530.png" alt="$$ \mathbf{p}_4 \sim \mathcal{N}( (\mathbf{A}^+ \mathbf{B})_{(4,\cdot)},(\gamma \mathbf{A}^{+^T} \mathbf{A}^+)_{(4,\cdot)}) $$" style="width:164px;height:15px;">,</p><p>We compare the posterior agreement to alternative validation criteria such as BIC and AIC.</p><p>The performance of each validation criteria is summarized by the confusion matrices whose entries capture the frequency of estimated clusters (columns) for a range of actual number of clusters (rows).</p><h2 id="3">Input</h2><pre class="codeinput">nsamples = 300;
edges = [1:1:800];
K = 2:12;
nClusters = 2:7;
</pre><pre class="codeinput"><span class="comment">% interpolate seed coord</span>
seed_coords = importdata(<span class="string">'seed_coords.mat'</span>); seed_coords(:,2) = 500-seed_coords(:,2);
xyz = seed_coords';
[~,npts]=size(xyz);
xyzp=[];
<span class="keyword">for</span> k=1:2
    xyzp(k,:)=fnval(csaps(1:npts,xyz(k,:)),[1:0.1:npts]);
<span class="keyword">end</span>
tmp = randperm(size(xyzp,2)); tmp = tmp(1:200); tmp = sort(tmp);
seed_coords = xyzp(:,tmp)';
</pre><h2 id="5">Learn spline control points</h2><pre class="codeinput">line = importdata(<span class="string">'anatomy_lines.mat'</span>);
data = [];
<span class="keyword">for</span> i = 1:length(line)
    line_obs{i} = line{i}; line_obs{i}(:,2) = 500-line_obs{i}(:,2);
    distribution{i} = learn_spline(line_obs{i});
    ctrl_pts(:,:,i) = distribution{i}.ctrl_pts.mean;
<span class="keyword">end</span>

[ctrl_pts2,ctrl_pts] = rearrange_ctrl_pts(ctrl_pts);

seed_ctrl_pts = reshape(ctrl_pts(1,:,:),2,[])';
</pre><pre class="codeinput"><span class="comment">% Initialize confusion matrix</span>
confusion_matrix.PA = zeros(max(nClusters),max(K));
confusion_matrix.BIC = zeros(max(nClusters),max(K));
confusion_matrix.AIC = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence1 = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence2 = zeros(max(nClusters),max(K));
</pre><pre class="codeinput"><span class="comment">% start timer</span>
tic;
</pre><pre class="codeinput">n_experiments =  1;
<span class="keyword">for</span> n = 1:n_experiments
<span class="keyword">for</span>  ktrue = nClusters
</pre><pre class="codeinput">    clear <span class="string">fiber_assignment</span> <span class="string">connectivity_matrix</span> <span class="string">target_coord</span>
    tmp0 = round(size(ctrl_pts2,3)*dirichletRnd(1,ones(1,ktrue)./0.1)); j=0;
    tmp0(end) = tmp0(end) + (size(ctrl_pts2,3) - sum(tmp0));
    <span class="keyword">for</span> i = 1:ktrue
        <span class="keyword">for</span> m = 1:tmp0(i)
            j=j+1;
            fiber_assignment{i}(m) = j;
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    tmp = round(size(seed_coords,1)*dirichletRnd(1,ones(1,8)./0.1)); j=0; idx_start = 1; idx_end = 0;
    tmp(end) = tmp(end) + (size(seed_coords,1) - sum(tmp));
    seed_labels_true = ones(1,size(seed_coords,1));
    <span class="keyword">for</span> i = 1:length(tmp)-1
        idx_start = idx_start+tmp(i);
        idx_end = idx_start+tmp(i+1)-1;
        seed_labels_true(idx_start:idx_end) = i+1;
    <span class="keyword">end</span>
</pre><h2 id="10">Connectivity matrices</h2><pre class="codeinput">    h = setup_plots;
    colors = distinguishable_colors(length(fiber_assignment));

    <span class="keyword">for</span> m = 1:2
        <span class="keyword">for</span> i = 1:size(seed_coords,1)
            clear <span class="string">spline_sample</span>
            <span class="keyword">for</span> j = 1:nsamples
                label(i) = find(cellfun(@(x) ismember(seed_labels_true(i),x),fiber_assignment));
                prob = ones(1,length(fiber_assignment{label(i)}))./length(fiber_assignment{label(i)});
                sample_idx = discreternd(prob,1);
                sample_idx = fiber_assignment{label(i)}(sample_idx);
                seed_coord_ctrl_pts = ctrl_pts2(:,:,sample_idx);
                seed_coord_ctrl_pts(1,:) = seed_coords(i,:);
                spline_sample{j} = sample_spline(5e1,1e4,seed_coord_ctrl_pts,line_obs{sample_idx});
                target_coord{m}(j,:,i) = round(spline_sample{j}(end,:));

                s(i) = sample_idx;
            <span class="keyword">end</span>
            <span class="keyword">if</span> m==1
            hold <span class="string">on</span>; p{i} = plot(h{1},spline_sample{j}(:,1),spline_sample{j}(:,2),<span class="string">'LineWidth'</span>,1);
            <span class="keyword">end</span>
            connectivity_matrix{m}(i,:) = reshape(histcn(target_coord{m}(:,:,i),1:600,1:600),1,[]);
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="keyword">for</span> i = 1:length(p)
        p{i}.Color = colors(label(i),:);
    <span class="keyword">end</span>
    scatter(h{1},seed_coords(:,1),seed_coords(:,2),50,colors(label,:),<span class="string">'filled'</span>)

    <span class="keyword">for</span> i = 1:length(p)
        p2{i} = plot(h{2},p{i}.XData,p{i}.YData,<span class="string">'Color'</span>,[0.8,0.8,0.8],<span class="string">'LineWidth'</span>,1);
        hold <span class="string">on</span>
    <span class="keyword">end</span>
    scatter(h{2},seed_coords(:,1),seed_coords(:,2),50,[0,0,0],<span class="string">'filled'</span>)

    <span class="comment">% remove zero columns</span>
    siz = size(connectivity_matrix{1},2);
    rm_idx = sum(connectivity_matrix{1},1) + sum(connectivity_matrix{2},1) == 0;
    connectivity_matrix{1}(:,rm_idx) = [];
    connectivity_matrix{2}(:,rm_idx) = [];

    <span class="comment">% normalize connectivity matrix</span>
    <span class="comment">% data{1} = sparse(bsxfun(@rdivide,connectivity_matrix{1},sum(connectivity_matrix{1},2)));</span>
    <span class="comment">% data{2} = sparse(bsxfun(@rdivide,connectivity_matrix{2},sum(connectivity_matrix{2},2)));</span>
    data{1} = sparse(connectivity_matrix{1});
    data{2} = sparse(connectivity_matrix{2});

    <span class="comment">% display connectivity matrix</span>
    imagesc(h{3},flipdim(logical(connectivity_matrix{1}),1)); colormap(h{3},gray)
    h{3}.FontSize = 15; h{3}.XTick = []; h{3}.YTick = []; h{3}.Box = <span class="string">'on'</span>;
    h{3}.Title.String = <span class="string">'Connectivity matrix'</span>; h{3}.XLabel.String = <span class="string">'target voxels'</span>;
    h{3}.YLabel.String = <span class="string">'seed voxels'</span>;

    <span class="comment">% display dissimilarity matrix</span>
    <span class="comment">%dsim = flipdim(pdist2(data{1},data{2}),1);</span>
    dsim = pdist2(data{1},data{2});
    imagesc(h{4},flipdim(flipdim(dsim,1),2));
    h{4}.FontSize = 15; h{4}.XTick = []; h{4}.YTick = []; h{4}.Box = <span class="string">'on'</span>;
    h{4}.Title.String = <span class="string">'Dissimilarity matrix'</span>;
    h{4}.XLabel.String = <span class="string">'seed voxels'</span>; h{4}.YLabel.String = <span class="string">'seed voxels'</span>;
</pre><img vspace="5" hspace="5" src="context_connectivity_low_spread_01.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_03.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_05.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_07.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_09.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_11.png" style="width:1600px;height:950px;" alt=""> <pre class="codeinput">    <span class="comment">% Bayesian evidence hyperparameters</span>

    alpha{1} = full(sum(data{1},1));
    alpha{2} = full(sum(data{1},1))/100;
</pre><h2 id="12">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput">    <span class="comment">% Annealing settings</span>
    beta_init = 1e-4;     <span class="comment">% starting inverse temperature</span>
    beta_step = 1.1;      <span class="comment">% inverse temperature step</span>
    beta_stop = 1e-1;      <span class="comment">% stopping inverse temperature</span>
    perturb_sd = 1e-4;    <span class="comment">% centroid perturbation</span>

    <span class="comment">% Initialization of information content</span>
    info_content = zeros(1,max(K));

    <span class="keyword">for</span> k = K
</pre><pre class="codeinput">        <span class="comment">% Initialization of Gibbs distributions</span>
        gibbs_dist1 = ones(size(data{1},1),k) ./ k;
        gibbs_dist2 = ones(size(data{2},1),k) ./ k;

        <span class="comment">% Initialization of centroids</span>
        centroid1 = gibbs_dist1'*data{1};
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
        centroid1(centroid1==0) = eps;  <span class="comment">% smoothing</span>
        centroid2 = gibbs_dist2'*data{2};
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
        centroid2(centroid2==0) = eps;  <span class="comment">% smoothing</span>

        j = 0; beta = beta_init; inv_temp = []; <span class="comment">%subplot(2,3,4); cla</span>
        <span class="keyword">while</span> beta &lt;= beta_stop
</pre><h2 id="15">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="context_connectivity_low_spread_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">            centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); <span class="comment">% normalize</span>
            centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); <span class="comment">% normalize</span>
</pre><h2 id="16">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">            <span class="keyword">for</span> iter = 1:10
</pre><h2 id="17">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="context_connectivity_low_spread_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">                potential1 = -data{1} * log(centroid1)';
</pre><h2 id="18">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="context_connectivity_low_spread_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">                potential2 = -data{2} * log(centroid2)';
</pre><h2 id="19">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="context_connectivity_low_spread_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">                gibbs_dist1 = exp(-beta * potential1);
                partition_sum1 = sum(gibbs_dist1,2);
                gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

                <span class="comment">% avoid underflow</span>
                idx = find(partition_sum1==0);
                <span class="keyword">if</span> ~isempty(idx)
                    [~,min_cost_idx] = min(potential1(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                    gibbs_dist1(idx,:) = zeros(length(idx),k);
                    gibbs_dist1(max_ind) = 1;
                <span class="keyword">end</span>
</pre><h2 id="20">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="context_connectivity_low_spread_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">                gibbs_dist2 = exp(-beta * potential2);
                partition_sum2 = sum(gibbs_dist2,2);
                gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

                <span class="comment">% avoid underflow</span>
                idx = find(partition_sum2==0);
                <span class="keyword">if</span> ~isempty(idx)
                    [~,min_cost_idx] = min(potential2(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                    gibbs_dist2(idx,:) = zeros(length(idx),k);
                    gibbs_dist2(max_ind) = 1;
                <span class="keyword">end</span>
</pre><h2 id="21">Joint Gibbs distribution</h2><p>Maximum entropy distribution: <img src="context_connectivity_low_spread_eq01026291022421135752.png" alt="$p_{ik}^{(1,2)} = \exp \left(-\beta ( R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$" style="width:159px;height:20px;"></p><pre class="codeinput">                dist_joint = exp(-beta * (potential1 + potential2));
                joint_partition_sum = sum(dist_joint,2);
</pre><h2 id="22">Centroids for instance 1</h2><p>Probability prototype: <img src="context_connectivity_low_spread_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">                centroid1 = gibbs_dist1'*data{1};
                centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
                centroid1(centroid1==0) = eps;
</pre><h2 id="23">Centroids for instance 2</h2><p>Probability prototype: <img src="context_connectivity_low_spread_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">                centroid2 = gibbs_dist2'*data{2};
                centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
                centroid2(centroid2==0) = eps;
            <span class="keyword">end</span>
</pre><pre class="codeinput">            <span class="comment">% increase inverse temperature:</span>
            beta = beta * beta_step;
</pre><h2 id="25">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">            match_clusters_idx = munkres(pdist2(centroid1,centroid2));
            potential2=potential2(:,match_clusters_idx);
</pre><h2 id="26">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="context_connectivity_low_spread_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">            scaled_cost1 = -beta * potential1;
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost1 = max(scaled_cost1,[],2);
            log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="27">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="context_connectivity_low_spread_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">            scaled_cost2 = -beta * potential2;
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost2 = max(scaled_cost2,[],2);
            log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="28">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="context_connectivity_low_spread_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">            joint_scaled_cost = -beta * (potential1 + potential2);
            gibbs_dist_joint = bsxfun(@rdivide,joint_scaled_cost,sum(joint_scaled_cost,2));
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost3 = max(joint_scaled_cost,[],2);
            log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
</pre><pre class="codeinput">            j = j+1;
</pre><h2 id="30">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="context_connectivity_low_spread_eq05853161460555252620.png" alt="$GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:221px;height:14px;"></p><pre class="codeinput">            gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
                - log_partition_sum2) ./ size(partition_sum1,1);
</pre><pre class="codeinput">            <span class="comment">% pack</span>
            inv_temp(j) = beta;
            gibbs_dist_packed1{k}(:,:,j) = gibbs_dist1;
            gibbs_dist_packed2{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);
</pre><pre class="codeinput">        <span class="keyword">end</span>
</pre><h2 id="33">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="context_connectivity_low_spread_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">        gibbs_dist1 = round(gibbs_dist1);
        d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
        nTransformations = -d * log(d)';

        <span class="comment">% correct generalization capacity</span>
        gc{k} = gc{k}-log(k)+nTransformations;

        gc{k} = gc{k} * log2(exp(1));  <span class="comment">% transforming units from nats to bits</span>
</pre><h2 id="34">Information content</h2><p>Quality of algorithm: <img src="context_connectivity_low_spread_eq12990683164727386852.png" alt="$\mathcal{I} = \max_{\beta} GC(\beta)$" style="width:81px;height:12px;"></p><pre class="codeinput">        [info_content(k),max_gc_idx(k)] = max(gc{k});
</pre><h2 id="35">Bayesian Information Criterion (BIC)</h2><p><img src="context_connectivity_low_spread_eq09700827967035327629.png" alt="$BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$" style="width:164px;height:14px;"> where <img src="context_connectivity_low_spread_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="context_connectivity_low_spread_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">        cost = sum(sum(gibbs_dist1 .* (-data{1} * log(centroid1)')));
        BIC(k) = size(data{1},2) * k * log(size(data{1},1)) + 2 * cost;
</pre><h2 id="36">Akaike Information Criterion (AIC)</h2><p><img src="context_connectivity_low_spread_eq03357731097339886371.png" alt="$AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$" style="width:143px;height:14px;"> where <img src="context_connectivity_low_spread_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="context_connectivity_low_spread_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">        AIC(k) = 2 * size(data{1},2) * k + 2 * cost;
</pre><h2 id="37">Bayesian evidence</h2><p><img src="context_connectivity_low_spread_eq08403349875303284569.png" alt="$p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi) p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k + \mathbf{n}_k)}{B(\alpha_k)}$" style="width:220px;height:18px;"> where <img src="context_connectivity_low_spread_eq08059537060765087149.png" alt="$\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$" style="width:113px;height:11px;"> and <img src="context_connectivity_low_spread_eq07727016875268873876.png" alt="$n_{kj} := \sum_{i:c(i)=k} x_{ij}$" style="width:86px;height:14px;"></p><pre class="codeinput">        [~,labels] = max(gibbs_dist1,[],2);
        log_bayes_evidence{1}(k) = 0;
        log_bayes_evidence{2}(k) = 0;
        <span class="keyword">for</span> c = unique(labels)'
            ncj = full(sum(data{1}(labels==c,:),1));
            alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
            alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
            ncj(ncj==0) = [];

            log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) <span class="keyword">...</span>
                + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
            log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) <span class="keyword">...</span>
                + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
        <span class="keyword">end</span>
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="39">Results</h2><pre class="codeinput">    [ktrue2,centroid_labels] = display_result(info_content,gc,inv_temp,BIC,AIC,log_bayes_evidence,<span class="keyword">...</span>
        gibbs_dist_packed1,gibbs_dist_packed2,K,p2,h,seed_coords,data,fiber_assignment);
</pre><h2 id="40">Update confusion matrices</h2><pre class="codeinput">    confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) = confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) + 1;
    confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) = confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) + 1;
    confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) = confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) + 1;
    confusion_matrix.bayes_evidence1(ktrue2,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) = confusion_matrix.bayes_evidence1(ktrue,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) + 1;
    confusion_matrix.bayes_evidence2(ktrue2,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) = confusion_matrix.bayes_evidence2(ktrue,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) + 1;

    drawnow
</pre><img vspace="5" hspace="5" src="context_connectivity_low_spread_02.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_04.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_06.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_08.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_10.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_low_spread_12.png" style="width:1600px;height:950px;" alt=""> <pre class="codeinput"><span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2 id="42">Confusion Matrix</h2><pre class="codeinput">disp(<span class="string">'Confusion Matrices'</span>); disp(<span class="string">' '</span>);
T.PA = array2table(confusion_matrix.PA(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Posterior Agreement:'</span>); disp(T.PA);
T.BIC = array2table(confusion_matrix.BIC(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'BIC:'</span>); disp(T.BIC);
T.AIC = array2table(confusion_matrix.AIC(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'AIC:'</span>); disp(T.AIC);
T.bayes_evidence1 = array2table(confusion_matrix.bayes_evidence1(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Bayes_evidence 1:'</span>); disp(T.bayes_evidence1);
T.bayes_evidence2 = array2table(confusion_matrix.bayes_evidence2(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Bayes_evidence 2:'</span>); disp(T.bayes_evidence2);
</pre><pre class="codeoutput">Confusion Matrices
 
Posterior Agreement:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    0     1     0     0     0     0     0     0     0      0      0  
    k4    0     0     1     0     0     0     0     0     0      0      0  
    k5    0     0     0     1     0     0     0     0     0      0      0  
    k6    0     0     0     0     1     0     0     0     0      0      0  
    k7    0     0     0     0     0     1     0     0     0      0      0  

BIC:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    1     0     0     0     0     0     0     0     0      0      0  
    k4    1     0     0     0     0     0     0     0     0      0      0  
    k5    1     0     0     0     0     0     0     0     0      0      0  
    k6    1     0     0     0     0     0     0     0     0      0      0  
    k7    0     1     0     0     0     0     0     0     0      0      0  

AIC:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    0     1     0     0     0     0     0     0     0      0      0  
    k4    0     1     0     0     0     0     0     0     0      0      0  
    k5    0     0     0     1     0     0     0     0     0      0      0  
    k6    0     0     0     1     0     0     0     0     0      0      0  
    k7    0     0     1     0     0     0     0     0     0      0      0  

Bayes_evidence 1:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    0     0     0     0     0     0     0     0     0      0      1  
    k3    0     0     0     0     0     0     0     0     0      0      1  
    k4    0     0     0     1     0     0     0     0     0      0      0  
    k5    0     0     0     0     0     0     0     0     0      0      1  
    k6    0     0     0     0     0     0     0     0     0      1      0  
    k7    0     0     0     0     0     0     0     1     0      0      0  

Bayes_evidence 2:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    0     1     0     0     0     0     0     0     0      0      0  
    k4    0     0     1     0     0     0     0     0     0      0      0  
    k5    0     0     0     1     0     0     0     0     0      0      0  
    k6    0     0     0     1     0     0     0     0     0      0      0  
    k7    0     0     0     0     0     0     1     0     0      0      0  

</pre><pre class="codeinput"><span class="comment">% display runtime</span>
disp([<span class="string">'runtime = '</span> num2str(toc/60) <span class="string">' minutes'</span>]);
</pre><pre class="codeoutput">runtime = 10.463 minutes
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (Low Fiber Spread)
% *Author: Nico Stephan Gorbach* ,
% Institute of Machine Learning, ETHZ,
% email: nico.gorbach@inf.ethz.ch
%
% Implementation of " *Pipeline Validation for Connectivity-based Cortex Parcellation* 
% " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann 
%
% Simulation for validation by posterior agreeement for clustering in an
% anatomical context
%
%%
clear all; close all; clc

%% Introduction
% This instructional code validates clustering in an anatomical context by simulating connectivity scores from 
% a seed region in the postcentral gyrus. Given a template connectivity structure we can describe the fibers underlying 
% that connectivity structure by Bézier curves. Bézier curves are defined
% by a set of control points $\mathbf{P}$:
%
% $$ \mathbf{B} = \mathbf{A}(\mathbf{t}) ~ \mathbf{P} + \epsilon,
% \qquad \epsilon \sim \mathcal{N}(\mathbf{0},\gamma \mathbf{I} ), $$
%
% where $\mathbf{t}$ is a vector of linearly spaced scalar values ranging from $0$ to $1$ and $\gamma$ 
% determines the spread of the fiber samples. For a cubic Bézier curve,
% matrix $\mathbf{A}$ is given by:
%
% $$ \mathbf{A}(\mathbf{t}) = (\mathbf{1} - \mathbf{t})^3, 3 (\mathbf{1} - \mathbf{t})^2 \mathbf{t}, 3(\mathbf{1}-\mathbf{t})\mathbf{t}^2, \mathbf{t}^3 ) $$. 
%
% Since Bézier curves are linear in the control points $\mathbf{P}$, we can determine the control points by multiplying 
% the left hand-side of equation \ref{eqn:bezier_curve} by the pseudo-inverse 
% $\mathbf{A}$ (i.e. $\mathbf{P} = \mathbf{A}^+ ( \mathbf{B} - \epsilon )$). Notice that the first 
% and last control points $\mathbf{p}_1,\mathbf{p}_4 \in \mathbf{P}$ mark the start and end of the Bézier curve, respectively. 
% We can therefore determine the connectivity scores by sampling from the marginal distribution of $\mathbf{p}_4$:
%
% $$ \mathbf{p}_4 \sim \mathcal{N}( (\mathbf{A}^+ \mathbf{B})_{(4,\cdot)},(\gamma \mathbf{A}^{+^T} \mathbf{A}^+)_{(4,\cdot)}) $$,
%
% We compare the posterior agreement to alternative validation criteria
% such as BIC and AIC.
%
% The performance of each validation criteria is summarized by the confusion matrices whose entries capture the frequency of estimated clusters (columns) 
% for a range of actual number of clusters (rows).

%% Input
nsamples = 300;
edges = [1:1:800];
K = 2:12;
nClusters = 2:7;
%%

% interpolate seed coord
seed_coords = importdata('seed_coords.mat'); seed_coords(:,2) = 500-seed_coords(:,2);
xyz = seed_coords';
[~,npts]=size(xyz);
xyzp=[];
for k=1:2
    xyzp(k,:)=fnval(csaps(1:npts,xyz(k,:)),[1:0.1:npts]);
end
tmp = randperm(size(xyzp,2)); tmp = tmp(1:200); tmp = sort(tmp);
seed_coords = xyzp(:,tmp)';

%% Learn spline control points
line = importdata('anatomy_lines.mat');
data = [];
for i = 1:length(line)
    line_obs{i} = line{i}; line_obs{i}(:,2) = 500-line_obs{i}(:,2);
    distribution{i} = learn_spline(line_obs{i});
    ctrl_pts(:,:,i) = distribution{i}.ctrl_pts.mean;
end

[ctrl_pts2,ctrl_pts] = rearrange_ctrl_pts(ctrl_pts);

seed_ctrl_pts = reshape(ctrl_pts(1,:,:),2,[])';

%%

% Initialize confusion matrix
confusion_matrix.PA = zeros(max(nClusters),max(K));
confusion_matrix.BIC = zeros(max(nClusters),max(K));
confusion_matrix.AIC = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence1 = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence2 = zeros(max(nClusters),max(K));

%%

% start timer
tic;

%%
n_experiments =  1;
for n = 1:n_experiments
for  ktrue = nClusters
    clear fiber_assignment connectivity_matrix target_coord
    tmp0 = round(size(ctrl_pts2,3)*dirichletRnd(1,ones(1,ktrue)./0.1)); j=0;
    tmp0(end) = tmp0(end) + (size(ctrl_pts2,3) - sum(tmp0));
    for i = 1:ktrue
        for m = 1:tmp0(i)
            j=j+1;
            fiber_assignment{i}(m) = j;
        end
    end
    
    tmp = round(size(seed_coords,1)*dirichletRnd(1,ones(1,8)./0.1)); j=0; idx_start = 1; idx_end = 0;
    tmp(end) = tmp(end) + (size(seed_coords,1) - sum(tmp));
    seed_labels_true = ones(1,size(seed_coords,1));
    for i = 1:length(tmp)-1
        idx_start = idx_start+tmp(i);
        idx_end = idx_start+tmp(i+1)-1;
        seed_labels_true(idx_start:idx_end) = i+1;
    end
    
     %% Connectivity matrices
    
    h = setup_plots;
    colors = distinguishable_colors(length(fiber_assignment));
    
    for m = 1:2
        for i = 1:size(seed_coords,1)
            clear spline_sample
            for j = 1:nsamples
                label(i) = find(cellfun(@(x) ismember(seed_labels_true(i),x),fiber_assignment));
                prob = ones(1,length(fiber_assignment{label(i)}))./length(fiber_assignment{label(i)});
                sample_idx = discreternd(prob,1);
                sample_idx = fiber_assignment{label(i)}(sample_idx);
                seed_coord_ctrl_pts = ctrl_pts2(:,:,sample_idx);
                seed_coord_ctrl_pts(1,:) = seed_coords(i,:);
                spline_sample{j} = sample_spline(5e1,1e4,seed_coord_ctrl_pts,line_obs{sample_idx});
                target_coord{m}(j,:,i) = round(spline_sample{j}(end,:));
                
                s(i) = sample_idx;
            end
            if m==1
            hold on; p{i} = plot(h{1},spline_sample{j}(:,1),spline_sample{j}(:,2),'LineWidth',1);
            end
            connectivity_matrix{m}(i,:) = reshape(histcn(target_coord{m}(:,:,i),1:600,1:600),1,[]);
        end
    end
    
    for i = 1:length(p)
        p{i}.Color = colors(label(i),:);
    end
    scatter(h{1},seed_coords(:,1),seed_coords(:,2),50,colors(label,:),'filled')
    
    for i = 1:length(p)
        p2{i} = plot(h{2},p{i}.XData,p{i}.YData,'Color',[0.8,0.8,0.8],'LineWidth',1);
        hold on
    end
    scatter(h{2},seed_coords(:,1),seed_coords(:,2),50,[0,0,0],'filled')
    
    % remove zero columns
    siz = size(connectivity_matrix{1},2);
    rm_idx = sum(connectivity_matrix{1},1) + sum(connectivity_matrix{2},1) == 0;
    connectivity_matrix{1}(:,rm_idx) = [];
    connectivity_matrix{2}(:,rm_idx) = [];
    
    % normalize connectivity matrix
    % data{1} = sparse(bsxfun(@rdivide,connectivity_matrix{1},sum(connectivity_matrix{1},2)));
    % data{2} = sparse(bsxfun(@rdivide,connectivity_matrix{2},sum(connectivity_matrix{2},2)));
    data{1} = sparse(connectivity_matrix{1});
    data{2} = sparse(connectivity_matrix{2});
    
    % display connectivity matrix
    imagesc(h{3},flipdim(logical(connectivity_matrix{1}),1)); colormap(h{3},gray)
    h{3}.FontSize = 15; h{3}.XTick = []; h{3}.YTick = []; h{3}.Box = 'on';
    h{3}.Title.String = 'Connectivity matrix'; h{3}.XLabel.String = 'target voxels';
    h{3}.YLabel.String = 'seed voxels';
    
    % display dissimilarity matrix
    %dsim = flipdim(pdist2(data{1},data{2}),1);
    dsim = pdist2(data{1},data{2});
    imagesc(h{4},flipdim(flipdim(dsim,1),2));
    h{4}.FontSize = 15; h{4}.XTick = []; h{4}.YTick = []; h{4}.Box = 'on';
    h{4}.Title.String = 'Dissimilarity matrix';
    h{4}.XLabel.String = 'seed voxels'; h{4}.YLabel.String = 'seed voxels';
    
    %%
    
    % Bayesian evidence hyperparameters
    
    alpha{1} = full(sum(data{1},1));
    alpha{2} = full(sum(data{1},1))/100;
    
    %% Deterministic annealing
    % Determine global minimizer.
    
    % Annealing settings
    beta_init = 1e-4;     % starting inverse temperature
    beta_step = 1.1;      % inverse temperature step
    beta_stop = 1e-1;      % stopping inverse temperature
    perturb_sd = 1e-4;    % centroid perturbation
    
    % Initialization of information content
    info_content = zeros(1,max(K));
    
    for k = K
        % Initialization of Gibbs distributions
        gibbs_dist1 = ones(size(data{1},1),k) ./ k;
        gibbs_dist2 = ones(size(data{2},1),k) ./ k;
        
        % Initialization of centroids
        centroid1 = gibbs_dist1'*data{1};
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
        centroid1(centroid1==0) = eps;  % smoothing
        centroid2 = gibbs_dist2'*data{2};
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
        centroid2(centroid2==0) = eps;  % smoothing
        
        j = 0; beta = beta_init; inv_temp = []; %subplot(2,3,4); cla
        while beta <= beta_stop
            
            %%% Perturb centroids
            % Avoid local minimum by perturbing centroids:
            % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
            centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); % normalize
            centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); % normalize
            
            %% Expectation maximization
            % Iterate between determining Gibbs distributions and maximzing
            % variational lower bound w.r.t. centroids.
            for iter = 1:10
                
                %%% Costs for histogram clustering given instance 1
                % KL divergence between empirical probabilities (data) and
                % centroid probabilities (up to proportionality constant):
                % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
                potential1 = -data{1} * log(centroid1)';
                
                %%% Costs for histogram clustering given instance 2
                % KL divergence between empirical probabilities (data) and
                % centroid probabilities (up to proportionality constant):
                % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
                potential2 = -data{2} * log(centroid2)';
                
                %%% Gibbs distribution 1
                % Maximum entropy distribution:
                % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
                gibbs_dist1 = exp(-beta * potential1);
                partition_sum1 = sum(gibbs_dist1,2);
                gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
                
                % avoid underflow
                idx = find(partition_sum1==0);
                if ~isempty(idx)
                    [~,min_cost_idx] = min(potential1(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                    gibbs_dist1(idx,:) = zeros(length(idx),k);
                    gibbs_dist1(max_ind) = 1;
                end
                
                %%% Gibbs distribution 2
                % Maximum entropy distribution:
                % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
                gibbs_dist2 = exp(-beta * potential2);
                partition_sum2 = sum(gibbs_dist2,2);
                gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
                
                % avoid underflow
                idx = find(partition_sum2==0);
                if ~isempty(idx)
                    [~,min_cost_idx] = min(potential2(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                    gibbs_dist2(idx,:) = zeros(length(idx),k);
                    gibbs_dist2(max_ind) = 1;
                end
                
                %%% Joint Gibbs distribution
                % Maximum entropy distribution:
                % $p_{ik}^{(1,2)} = \exp \left(-\beta (
                % R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$
                dist_joint = exp(-beta * (potential1 + potential2));
                joint_partition_sum = sum(dist_joint,2);
                
                %%% Centroids for instance 1
                % Probability prototype:
                % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
                centroid1 = gibbs_dist1'*data{1};
                centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
                centroid1(centroid1==0) = eps;
                
                %%% Centroids for instance 2
                % Probability prototype:
                % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
                centroid2 = gibbs_dist2'*data{2};
                centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
                centroid2(centroid2==0) = eps;
            end
            
            %%
            
            % increase inverse temperature:
            beta = beta * beta_step;
            
            %%% Match clusters across data instances
            % Use Hungarian algorithm to match clusters.
            match_clusters_idx = munkres(pdist2(centroid1,centroid2));
            potential2=potential2(:,match_clusters_idx);
            
            %%% Log partition sum for instance 1
            % Determine log partition sum while avoiding underflow:
            % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
            scaled_cost1 = -beta * potential1;
            % log-sum-exp trick to prevent underflow
            max_scaled_cost1 = max(scaled_cost1,[],2);
            log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
            
            %%% Log partition sum for instance 2
            % Determine log partition sum while avoiding underflow:
            % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
            scaled_cost2 = -beta * potential2;
            % log-sum-exp trick to prevent underflow
            max_scaled_cost2 = max(scaled_cost2,[],2);
            log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
            
            %%% Joint log partition sum
            % Determine joint log partition sum while avoiding underflow:
            % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
            joint_scaled_cost = -beta * (potential1 + potential2);
            gibbs_dist_joint = bsxfun(@rdivide,joint_scaled_cost,sum(joint_scaled_cost,2));
            % log-sum-exp trick to prevent underflow
            max_scaled_cost3 = max(joint_scaled_cost,[],2);
            log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
            
            %%
            
            j = j+1;
            
            %%% Generalization capacity
            % Resolution of the hypothesis space:
            % $GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
            gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 ...
                - log_partition_sum2) ./ size(partition_sum1,1);
            
            %%
            
            % pack
            inv_temp(j) = beta;
            gibbs_dist_packed1{k}(:,:,j) = gibbs_dist1;
            gibbs_dist_packed2{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);

        end
        
        %% Number of equivariant transformations
        % Richness of the hypothesis space:
        % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
        gibbs_dist1 = round(gibbs_dist1);
        d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
        nTransformations = -d * log(d)';
        
        % correct generalization capacity
        gc{k} = gc{k}-log(k)+nTransformations;
        
        gc{k} = gc{k} * log2(exp(1));  % transforming units from nats to bits
        
        %% Information content
        % Quality of algorithm:
        % $\mathcal{I} = \max_{\beta} GC(\beta)$
        [info_content(k),max_gc_idx(k)] = max(gc{k});
        
        %% Bayesian Information Criterion (BIC)
        % $BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the
        % number of bins and $c^{\perp}$ is the empirical risk minimizer
        
        cost = sum(sum(gibbs_dist1 .* (-data{1} * log(centroid1)')));
        BIC(k) = size(data{1},2) * k * log(size(data{1},1)) + 2 * cost;
        
        %% Akaike Information Criterion (AIC)
        % $AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the
        % number of bins and $c^{\perp}$ is the empirical risk minimizer
        
        AIC(k) = 2 * size(data{1},2) * k + 2 * cost;
        
        %% Bayesian evidence
        % $p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi)
        % p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k +
        % \mathbf{n}_k)}{B(\alpha_k)}$ where $\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$ 
        % and $n_{kj} := \sum_{i:c(i)=k} x_{ij}$
        
        [~,labels] = max(gibbs_dist1,[],2);
        log_bayes_evidence{1}(k) = 0;
        log_bayes_evidence{2}(k) = 0;
        for c = unique(labels)'
            ncj = full(sum(data{1}(labels==c,:),1));
            alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
            alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
            ncj(ncj==0) = [];
            
            log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) ...
                + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
            log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) ...
                + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
        end
    end
    
    %% Results
    [ktrue2,centroid_labels] = display_result(info_content,gc,inv_temp,BIC,AIC,log_bayes_evidence,...
        gibbs_dist_packed1,gibbs_dist_packed2,K,p2,h,seed_coords,data,fiber_assignment);

    %%% Update confusion matrices
    
    confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) = confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) + 1;
    confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) = confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) + 1;
    confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) = confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) + 1;
    confusion_matrix.bayes_evidence1(ktrue2,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) = confusion_matrix.bayes_evidence1(ktrue,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) + 1;
    confusion_matrix.bayes_evidence2(ktrue2,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) = confusion_matrix.bayes_evidence2(ktrue,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) + 1;
    
    drawnow
end
end
%% Confusion Matrix
disp('Confusion Matrices'); disp(' ');
T.PA = array2table(confusion_matrix.PA(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Posterior Agreement:'); disp(T.PA);
T.BIC = array2table(confusion_matrix.BIC(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('BIC:'); disp(T.BIC);
T.AIC = array2table(confusion_matrix.AIC(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('AIC:'); disp(T.AIC);
T.bayes_evidence1 = array2table(confusion_matrix.bayes_evidence1(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Bayes_evidence 1:'); disp(T.bayes_evidence1);
T.bayes_evidence2 = array2table(confusion_matrix.bayes_evidence2(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Bayes_evidence 2:'); disp(T.bayes_evidence2);
%%

% display runtime
disp(['runtime = ' num2str(toc/60) ' minutes']);

##### SOURCE END #####
--></body></html>




<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (High Fiber Spread)</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-11-08"><meta name="DC.source" content="context_connectivity_high_spread.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (High Fiber Spread)</h1><!--introduction--><p><b>Author: Nico Stephan Gorbach</b> , Institute of Machine Learning, ETHZ, email: <a href="mailto:nico.gorbach@inf.ethz.ch">nico.gorbach@inf.ethz.ch</a></p><p>Implementation of " <b>Pipeline Validation for Connectivity-based Cortex Parcellation</b> " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann</p><p>Simulation for validation by posterior agreeement for clustering in an anatomical context</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#3">Input</a></li><li><a href="#5">Learn spline control points</a></li><li><a href="#10">Connectivity matrices</a></li><li><a href="#12">Deterministic annealing</a></li><li><a href="#15">Perturb centroids</a></li><li><a href="#16">Expectation maximization</a></li><li><a href="#17">Costs for histogram clustering given instance 1</a></li><li><a href="#18">Costs for histogram clustering given instance 2</a></li><li><a href="#19">Gibbs distribution 1</a></li><li><a href="#20">Gibbs distribution 2</a></li><li><a href="#21">Joint Gibbs distribution</a></li><li><a href="#22">Centroids for instance 1</a></li><li><a href="#23">Centroids for instance 2</a></li><li><a href="#25">Match clusters across data instances</a></li><li><a href="#26">Log partition sum for instance 1</a></li><li><a href="#27">Log partition sum for instance 2</a></li><li><a href="#28">Joint log partition sum</a></li><li><a href="#30">Generalization capacity</a></li><li><a href="#33">Number of equivariant transformations</a></li><li><a href="#34">Information content</a></li><li><a href="#35">Bayesian Information Criterion (BIC)</a></li><li><a href="#36">Akaike Information Criterion (AIC)</a></li><li><a href="#37">Bayesian evidence</a></li><li><a href="#39">Results</a></li><li><a href="#40">Update confusion matrices</a></li><li><a href="#42">Confusion Matrix</a></li></ul></div><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>; clc
</pre><h2 id="2">Introduction</h2><p>This instructional code validates clustering in an anatomical context by simulating connectivity scores from a seed region in the postcentral gyrus. Given a template connectivity structure we can describe the fibers underlying that connectivity structure by B&eacute;zier curves. B&eacute;zier curves are defined by a set of control points <img src="context_connectivity_high_spread_eq04341845410467366200.png" alt="$\mathbf{P}$" style="width:8px;height:8px;">:</p><p><img src="context_connectivity_high_spread_eq10989988532119641984.png" alt="$$ \mathbf{B} = \mathbf{A}(\mathbf{t}) ~ \mathbf{P} + \epsilon,&#xA;\qquad \epsilon \sim \mathcal{N}(\mathbf{0},\gamma \mathbf{I} ), $$" style="width:167px;height:12px;"></p><p>where <img src="context_connectivity_high_spread_eq11141810138104989172.png" alt="$\mathbf{t}$" style="width:5px;height:8px;"> is a vector of linearly spaced scalar values ranging from <img src="context_connectivity_high_spread_eq00202142981986870057.png" alt="$0$" style="width:5px;height:8px;"> to <img src="context_connectivity_high_spread_eq18395870634560867587.png" alt="$1$" style="width:4px;height:8px;"> and <img src="context_connectivity_high_spread_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;"> determines the spread of the fiber samples. For a cubic B&eacute;zier curve, matrix <img src="context_connectivity_high_spread_eq04088523938794390934.png" alt="$\mathbf{A}$" style="width:9px;height:8px;"> is given by:</p><p><img src="context_connectivity_high_spread_eq07893260694569411196.png" alt="$$ \mathbf{A}(\mathbf{t}) = (\mathbf{1} - \mathbf{t})^3, 3 (\mathbf{1} - \mathbf{t})^2 \mathbf{t}, 3(\mathbf{1}-\mathbf{t})\mathbf{t}^2, \mathbf{t}^3 ) $$" style="width:199px;height:13px;">.</p><p>Since B&eacute;zier curves are linear in the control points <img src="context_connectivity_high_spread_eq04341845410467366200.png" alt="$\mathbf{P}$" style="width:8px;height:8px;">, we can determine the control points by multiplying the left hand-side of equation \ref{eqn:bezier_curve} by the pseudo-inverse <img src="context_connectivity_high_spread_eq04088523938794390934.png" alt="$\mathbf{A}$" style="width:9px;height:8px;"> (i.e. <img src="context_connectivity_high_spread_eq05482758431359188095.png" alt="$\mathbf{P} = \mathbf{A}^+ ( \mathbf{B} - \epsilon )$" style="width:74px;height:12px;">). Notice that the first and last control points <img src="context_connectivity_high_spread_eq16182667400455286292.png" alt="$\mathbf{p}_1,\mathbf{p}_4 \in \mathbf{P}$" style="width:49px;height:10px;"> mark the start and end of the B&eacute;zier curve, respectively. We can therefore determine the connectivity scores by sampling from the marginal distribution of <img src="context_connectivity_high_spread_eq12997305262010432456.png" alt="$\mathbf{p}_4$" style="width:11px;height:8px;">:</p><p><img src="context_connectivity_high_spread_eq03423795116937024530.png" alt="$$ \mathbf{p}_4 \sim \mathcal{N}( (\mathbf{A}^+ \mathbf{B})_{(4,\cdot)},(\gamma \mathbf{A}^{+^T} \mathbf{A}^+)_{(4,\cdot)}) $$" style="width:164px;height:15px;">,</p><p>We compare the posterior agreement to alternative validation criteria such as BIC and AIC.</p><p>The performance of each validation criteria is summarized by the confusion matrices whose entries capture the frequency of estimated clusters (columns) for a range of actual number of clusters (rows).</p><h2 id="3">Input</h2><pre class="codeinput">nsamples = 300;
edges = [1:1:800];
K = 2:12;
nClusters = 2:7;
</pre><pre class="codeinput"><span class="comment">% interpolate seed coord</span>
seed_coords = importdata(<span class="string">'seed_coords.mat'</span>); seed_coords(:,2) = 500-seed_coords(:,2);
xyz = seed_coords';
[~,npts]=size(xyz);
xyzp=[];
<span class="keyword">for</span> k=1:2
    xyzp(k,:)=fnval(csaps(1:npts,xyz(k,:)),[1:0.1:npts]);
<span class="keyword">end</span>
tmp = randperm(size(xyzp,2)); tmp = tmp(1:200); tmp = sort(tmp);
seed_coords = xyzp(:,tmp)';
</pre><h2 id="5">Learn spline control points</h2><pre class="codeinput">line = importdata(<span class="string">'anatomy_lines.mat'</span>);
data = [];
<span class="keyword">for</span> i = 1:length(line)
    line_obs{i} = line{i}; line_obs{i}(:,2) = 500-line_obs{i}(:,2);
    distribution{i} = learn_spline(line_obs{i});
    ctrl_pts(:,:,i) = distribution{i}.ctrl_pts.mean;
<span class="keyword">end</span>

[ctrl_pts2,ctrl_pts] = rearrange_ctrl_pts(ctrl_pts);

seed_ctrl_pts = reshape(ctrl_pts(1,:,:),2,[])';
</pre><pre class="codeinput"><span class="comment">% Initialize confusion matrix</span>
confusion_matrix.PA = zeros(max(nClusters),max(K));
confusion_matrix.BIC = zeros(max(nClusters),max(K));
confusion_matrix.AIC = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence1 = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence2 = zeros(max(nClusters),max(K));
</pre><pre class="codeinput"><span class="comment">% start timer</span>
tic;
</pre><pre class="codeinput">n_experiments =  1;
<span class="keyword">for</span> n = 1:n_experiments
<span class="keyword">for</span>  ktrue = nClusters
</pre><pre class="codeinput">    clear <span class="string">fiber_assignment</span> <span class="string">connectivity_matrix</span> <span class="string">target_coord</span>
    tmp0 = round(size(ctrl_pts2,3)*dirichletRnd(1,ones(1,ktrue)./0.1)); j=0;
    tmp0(end) = tmp0(end) + (size(ctrl_pts2,3) - sum(tmp0));
    <span class="keyword">for</span> i = 1:ktrue
        <span class="keyword">for</span> m = 1:tmp0(i)
            j=j+1;
            fiber_assignment{i}(m) = j;
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    tmp = round(size(seed_coords,1)*dirichletRnd(1,ones(1,8)./0.1)); j=0; idx_start = 1; idx_end = 0;
    tmp(end) = tmp(end) + (size(seed_coords,1) - sum(tmp));
    seed_labels_true = ones(1,size(seed_coords,1));
    <span class="keyword">for</span> i = 1:length(tmp)-1
        idx_start = idx_start+tmp(i);
        idx_end = idx_start+tmp(i+1)-1;
        seed_labels_true(idx_start:idx_end) = i+1;
    <span class="keyword">end</span>
</pre><h2 id="10">Connectivity matrices</h2><pre class="codeinput">    h = setup_plots;
    colors = distinguishable_colors(length(fiber_assignment));

    <span class="keyword">for</span> m = 1:2
        <span class="keyword">for</span> i = 1:size(seed_coords,1)
            clear <span class="string">spline_sample</span>
            <span class="keyword">for</span> j = 1:nsamples
                label(i) = find(cellfun(@(x) ismember(seed_labels_true(i),x),fiber_assignment));
                prob = ones(1,length(fiber_assignment{label(i)}))./length(fiber_assignment{label(i)});
                sample_idx = discreternd(prob,1);
                sample_idx = fiber_assignment{label(i)}(sample_idx);
                seed_coord_ctrl_pts = ctrl_pts2(:,:,sample_idx);
                seed_coord_ctrl_pts(1,:) = seed_coords(i,:);
                spline_sample{j} = sample_spline(5e2,1e4,seed_coord_ctrl_pts,line_obs{sample_idx});
                target_coord{m}(j,:,i) = round(spline_sample{j}(end,:));

                s(i) = sample_idx;
            <span class="keyword">end</span>
            <span class="keyword">if</span> m==1
            hold <span class="string">on</span>; p{i} = plot(h{1},spline_sample{j}(:,1),spline_sample{j}(:,2),<span class="string">'LineWidth'</span>,1);
            <span class="keyword">end</span>
            connectivity_matrix{m}(i,:) = reshape(histcn(target_coord{m}(:,:,i),1:600,1:600),1,[]);
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="keyword">for</span> i = 1:length(p)
        p{i}.Color = colors(label(i),:);
    <span class="keyword">end</span>
    scatter(h{1},seed_coords(:,1),seed_coords(:,2),50,colors(label,:),<span class="string">'filled'</span>)

    <span class="keyword">for</span> i = 1:length(p)
        p2{i} = plot(h{2},p{i}.XData,p{i}.YData,<span class="string">'Color'</span>,[0.8,0.8,0.8],<span class="string">'LineWidth'</span>,1);
        hold <span class="string">on</span>
    <span class="keyword">end</span>
    scatter(h{2},seed_coords(:,1),seed_coords(:,2),50,[0,0,0],<span class="string">'filled'</span>)

    <span class="comment">% remove zero columns</span>
    siz = size(connectivity_matrix{1},2);
    rm_idx = sum(connectivity_matrix{1},1) + sum(connectivity_matrix{2},1) == 0;
    connectivity_matrix{1}(:,rm_idx) = [];
    connectivity_matrix{2}(:,rm_idx) = [];

    <span class="comment">% normalize connectivity matrix</span>
    <span class="comment">% data{1} = sparse(bsxfun(@rdivide,connectivity_matrix{1},sum(connectivity_matrix{1},2)));</span>
    <span class="comment">% data{2} = sparse(bsxfun(@rdivide,connectivity_matrix{2},sum(connectivity_matrix{2},2)));</span>
    data{1} = sparse(connectivity_matrix{1});
    data{2} = sparse(connectivity_matrix{2});

    <span class="comment">% display connectivity matrix</span>
    imagesc(h{3},flipdim(logical(connectivity_matrix{1}),1)); colormap(h{3},gray)
    h{3}.FontSize = 15; h{3}.XTick = []; h{3}.YTick = []; h{3}.Box = <span class="string">'on'</span>;
    h{3}.Title.String = <span class="string">'Connectivity matrix'</span>; h{3}.XLabel.String = <span class="string">'target voxels'</span>;
    h{3}.YLabel.String = <span class="string">'seed voxels'</span>;

    <span class="comment">% display dissimilarity matrix</span>
    <span class="comment">%dsim = flipdim(pdist2(data{1},data{2}),1);</span>
    dsim = pdist2(data{1},data{2});
    imagesc(h{4},flipdim(flipdim(dsim,1),2));
    h{4}.FontSize = 15; h{4}.XTick = []; h{4}.YTick = []; h{4}.Box = <span class="string">'on'</span>;
    h{4}.Title.String = <span class="string">'Dissimilarity matrix'</span>;
    h{4}.XLabel.String = <span class="string">'seed voxels'</span>; h{4}.YLabel.String = <span class="string">'seed voxels'</span>;
</pre><img vspace="5" hspace="5" src="context_connectivity_high_spread_01.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_03.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_05.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_07.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_09.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_11.png" style="width:1600px;height:950px;" alt=""> <pre class="codeinput">    <span class="comment">% Bayesian evidence hyperparameters</span>

    alpha{1} = full(sum(data{1},1));
    alpha{2} = full(sum(data{1},1))/100;
</pre><h2 id="12">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput">    <span class="comment">% Annealing settings</span>
    beta_init = 1e-4;     <span class="comment">% starting inverse temperature</span>
    beta_step = 1.1;      <span class="comment">% inverse temperature step</span>
    beta_stop = 1e-1;      <span class="comment">% stopping inverse temperature</span>
    perturb_sd = 1e-4;    <span class="comment">% centroid perturbation</span>

    <span class="comment">% Initialization of information content</span>
    info_content = zeros(1,max(K));

    <span class="keyword">for</span> k = K
</pre><pre class="codeinput">        <span class="comment">% Initialization of Gibbs distributions</span>
        gibbs_dist1 = ones(size(data{1},1),k) ./ k;
        gibbs_dist2 = ones(size(data{2},1),k) ./ k;

        <span class="comment">% Initialization of centroids</span>
        centroid1 = gibbs_dist1'*data{1};
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
        centroid1(centroid1==0) = eps;  <span class="comment">% smoothing</span>
        centroid2 = gibbs_dist2'*data{2};
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
        centroid2(centroid2==0) = eps;  <span class="comment">% smoothing</span>

        j = 0; beta = beta_init; inv_temp = []; <span class="comment">%subplot(2,3,4); cla</span>
        <span class="keyword">while</span> beta &lt;= beta_stop
</pre><h2 id="15">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="context_connectivity_high_spread_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">            centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); <span class="comment">% normalize</span>
            centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); <span class="comment">% normalize</span>
</pre><h2 id="16">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">            <span class="keyword">for</span> iter = 1:10
</pre><h2 id="17">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="context_connectivity_high_spread_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">                potential1 = -data{1} * log(centroid1)';
</pre><h2 id="18">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="context_connectivity_high_spread_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">                potential2 = -data{2} * log(centroid2)';
</pre><h2 id="19">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="context_connectivity_high_spread_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">                gibbs_dist1 = exp(-beta * potential1);
                partition_sum1 = sum(gibbs_dist1,2);
                gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

                <span class="comment">% avoid underflow</span>
                idx = find(partition_sum1==0);
                <span class="keyword">if</span> ~isempty(idx)
                    [~,min_cost_idx] = min(potential1(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                    gibbs_dist1(idx,:) = zeros(length(idx),k);
                    gibbs_dist1(max_ind) = 1;
                <span class="keyword">end</span>
</pre><h2 id="20">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="context_connectivity_high_spread_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">                gibbs_dist2 = exp(-beta * potential2);
                partition_sum2 = sum(gibbs_dist2,2);
                gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

                <span class="comment">% avoid underflow</span>
                idx = find(partition_sum2==0);
                <span class="keyword">if</span> ~isempty(idx)
                    [~,min_cost_idx] = min(potential2(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                    gibbs_dist2(idx,:) = zeros(length(idx),k);
                    gibbs_dist2(max_ind) = 1;
                <span class="keyword">end</span>
</pre><h2 id="21">Joint Gibbs distribution</h2><p>Maximum entropy distribution: <img src="context_connectivity_high_spread_eq01026291022421135752.png" alt="$p_{ik}^{(1,2)} = \exp \left(-\beta ( R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$" style="width:159px;height:20px;"></p><pre class="codeinput">                dist_joint = exp(-beta * (potential1 + potential2));
                joint_partition_sum = sum(dist_joint,2);
</pre><h2 id="22">Centroids for instance 1</h2><p>Probability prototype: <img src="context_connectivity_high_spread_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">                centroid1 = gibbs_dist1'*data{1};
                centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
                centroid1(centroid1==0) = eps;
</pre><h2 id="23">Centroids for instance 2</h2><p>Probability prototype: <img src="context_connectivity_high_spread_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">                centroid2 = gibbs_dist2'*data{2};
                centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
                centroid2(centroid2==0) = eps;
            <span class="keyword">end</span>
</pre><pre class="codeinput">            <span class="comment">% increase inverse temperature:</span>
            beta = beta * beta_step;
</pre><h2 id="25">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">            match_clusters_idx = munkres(pdist2(centroid1,centroid2));
            potential2=potential2(:,match_clusters_idx);
</pre><h2 id="26">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="context_connectivity_high_spread_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">            scaled_cost1 = -beta * potential1;
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost1 = max(scaled_cost1,[],2);
            log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="27">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="context_connectivity_high_spread_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">            scaled_cost2 = -beta * potential2;
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost2 = max(scaled_cost2,[],2);
            log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="28">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="context_connectivity_high_spread_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">            joint_scaled_cost = -beta * (potential1 + potential2);
            gibbs_dist_joint = bsxfun(@rdivide,joint_scaled_cost,sum(joint_scaled_cost,2));
            <span class="comment">% log-sum-exp trick to prevent underflow</span>
            max_scaled_cost3 = max(joint_scaled_cost,[],2);
            log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
</pre><pre class="codeinput">            j = j+1;
</pre><h2 id="30">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="context_connectivity_high_spread_eq05853161460555252620.png" alt="$GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:221px;height:14px;"></p><pre class="codeinput">            gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
                - log_partition_sum2) ./ size(partition_sum1,1);
</pre><pre class="codeinput">            <span class="comment">% pack</span>
            inv_temp(j) = beta;
            gibbs_dist_packed1{k}(:,:,j) = gibbs_dist1;
            gibbs_dist_packed2{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);
</pre><pre class="codeinput">        <span class="keyword">end</span>
</pre><h2 id="33">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="context_connectivity_high_spread_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">        gibbs_dist1 = round(gibbs_dist1);
        d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
        nTransformations = -d * log(d)';

        <span class="comment">% correct generalization capacity</span>
        gc{k} = gc{k}-log(k)+nTransformations;

        gc{k} = gc{k} * log2(exp(1));  <span class="comment">% transforming units from nats to bits</span>
</pre><h2 id="34">Information content</h2><p>Quality of algorithm: <img src="context_connectivity_high_spread_eq12990683164727386852.png" alt="$\mathcal{I} = \max_{\beta} GC(\beta)$" style="width:81px;height:12px;"></p><pre class="codeinput">        [info_content(k),max_gc_idx(k)] = max(gc{k});
</pre><h2 id="35">Bayesian Information Criterion (BIC)</h2><p><img src="context_connectivity_high_spread_eq09700827967035327629.png" alt="$BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$" style="width:164px;height:14px;"> where <img src="context_connectivity_high_spread_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="context_connectivity_high_spread_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">        cost = sum(sum(gibbs_dist1 .* (-data{1} * log(centroid1)')));
        BIC(k) = size(data{1},2) * k * log(size(data{1},1)) + 2 * cost;
</pre><h2 id="36">Akaike Information Criterion (AIC)</h2><p><img src="context_connectivity_high_spread_eq03357731097339886371.png" alt="$AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$" style="width:143px;height:14px;"> where <img src="context_connectivity_high_spread_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="context_connectivity_high_spread_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">        AIC(k) = 2 * size(data{1},2) * k + 2 * cost;
</pre><h2 id="37">Bayesian evidence</h2><p><img src="context_connectivity_high_spread_eq08403349875303284569.png" alt="$p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi) p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k + \mathbf{n}_k)}{B(\alpha_k)}$" style="width:220px;height:18px;"> where <img src="context_connectivity_high_spread_eq08059537060765087149.png" alt="$\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$" style="width:113px;height:11px;"> and <img src="context_connectivity_high_spread_eq07727016875268873876.png" alt="$n_{kj} := \sum_{i:c(i)=k} x_{ij}$" style="width:86px;height:14px;"></p><pre class="codeinput">        [~,labels] = max(gibbs_dist1,[],2);
        log_bayes_evidence{1}(k) = 0;
        log_bayes_evidence{2}(k) = 0;
        <span class="keyword">for</span> c = unique(labels)'
            ncj = full(sum(data{1}(labels==c,:),1));
            alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
            alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
            ncj(ncj==0) = [];

            log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) <span class="keyword">...</span>
                + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
            log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) <span class="keyword">...</span>
                + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
        <span class="keyword">end</span>
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="39">Results</h2><pre class="codeinput">    [ktrue2,centroid_labels] = display_result(info_content,gc,inv_temp,BIC,AIC,log_bayes_evidence,<span class="keyword">...</span>
        gibbs_dist_packed1,gibbs_dist_packed2,K,p2,h,seed_coords,data,fiber_assignment);
</pre><h2 id="40">Update confusion matrices</h2><pre class="codeinput">    confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) = confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) + 1;
    confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) = confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) + 1;
    confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) = confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) + 1;
    confusion_matrix.bayes_evidence1(ktrue2,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) = confusion_matrix.bayes_evidence1(ktrue,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) + 1;
    confusion_matrix.bayes_evidence2(ktrue2,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) = confusion_matrix.bayes_evidence2(ktrue,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) + 1;

    drawnow
</pre><img vspace="5" hspace="5" src="context_connectivity_high_spread_02.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_04.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_06.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_08.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_10.png" style="width:1600px;height:950px;" alt=""> <img vspace="5" hspace="5" src="context_connectivity_high_spread_12.png" style="width:1600px;height:950px;" alt=""> <pre class="codeinput"><span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2 id="42">Confusion Matrix</h2><pre class="codeinput">disp(<span class="string">'Confusion Matrices'</span>); disp(<span class="string">' '</span>);
T.PA = array2table(confusion_matrix.PA(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Posterior Agreement:'</span>); disp(T.PA);
T.BIC = array2table(confusion_matrix.BIC(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'BIC:'</span>); disp(T.BIC);
T.AIC = array2table(confusion_matrix.AIC(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'AIC:'</span>); disp(T.AIC);
T.bayes_evidence1 = array2table(confusion_matrix.bayes_evidence1(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Bayes_evidence 1:'</span>); disp(T.bayes_evidence1);
T.bayes_evidence2 = array2table(confusion_matrix.bayes_evidence2(2:end,2:end),<span class="string">'VariableNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>,<span class="string">'k8'</span>,<span class="string">'k9'</span>,<span class="string">'k10'</span>,<span class="string">'k11'</span>,<span class="string">'k12'</span>},<span class="string">'RowNames'</span>,{<span class="string">'k2'</span>,<span class="string">'k3'</span>,<span class="string">'k4'</span>,<span class="string">'k5'</span>,<span class="string">'k6'</span>,<span class="string">'k7'</span>});
disp(<span class="string">'Bayes_evidence 2:'</span>); disp(T.bayes_evidence2);
</pre><pre class="codeoutput">Confusion Matrices
 
Posterior Agreement:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    0     1     0     0     0     0     0     0     0      0      0  
    k4    0     0     2     0     0     0     0     0     0      0      0  
    k5    0     0     0     0     0     0     0     0     0      0      0  
    k6    0     0     0     0     2     0     0     0     0      0      0  
    k7    0     0     0     0     0     0     0     0     0      0      0  

BIC:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    1     0     0     0     0     0     0     0     0      0      0  
    k4    2     0     0     0     0     0     0     0     0      0      0  
    k5    0     0     0     0     0     0     0     0     0      0      0  
    k6    2     0     0     0     0     0     0     0     0      0      0  
    k7    0     0     0     0     0     0     0     0     0      0      0  

AIC:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    1     0     0     0     0     0     0     0     0      0      0  
    k3    1     0     0     0     0     0     0     0     0      0      0  
    k4    2     0     0     0     0     0     0     0     0      0      0  
    k5    0     0     0     0     0     0     0     0     0      0      0  
    k6    2     0     0     0     0     0     0     0     0      0      0  
    k7    0     0     0     0     0     0     0     0     0      0      0  

Bayes_evidence 1:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    0     0     0     0     0     0     0     0     0      0      1  
    k3    0     0     0     0     0     0     0     0     0      0      1  
    k4    0     0     0     0     0     0     0     0     0      1      1  
    k5    0     0     0     0     0     0     0     0     0      0      0  
    k6    0     0     0     0     0     0     0     0     0      1      0  
    k7    0     0     0     0     0     0     0     0     0      0      0  

Bayes_evidence 2:
          k2    k3    k4    k5    k6    k7    k8    k9    k10    k11    k12
          __    __    __    __    __    __    __    __    ___    ___    ___

    k2    0     0     0     0     0     0     0     0     0      0      1  
    k3    0     0     0     0     0     0     0     0     0      0      1  
    k4    0     0     0     0     0     0     0     0     0      1      1  
    k5    0     0     0     0     0     0     0     0     0      0      0  
    k6    0     0     0     0     0     0     0     0     0      0      1  
    k7    0     0     0     0     0     0     0     0     0      0      0  

</pre><pre class="codeinput"><span class="comment">% display runtime</span>
disp([<span class="string">'runtime = '</span> num2str(toc/60) <span class="string">' minutes'</span>]);
</pre><pre class="codeoutput">runtime = 16.9444 minutes
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% POSTERIOR AGREEMENT FOR CONNECTIVITY IN ANATOMICAL CONTEXT (High Fiber Spread)
% *Author: Nico Stephan Gorbach* ,
% Institute of Machine Learning, ETHZ,
% email: nico.gorbach@inf.ethz.ch
%
% Implementation of " *Pipeline Validation for Connectivity-based Cortex Parcellation* 
% " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann 
%
% Simulation for validation by posterior agreeement for clustering in an
% anatomical context
%
%%
clear all; close all; clc

%% Introduction
% This instructional code validates clustering in an anatomical context by simulating connectivity scores from 
% a seed region in the postcentral gyrus. Given a template connectivity structure we can describe the fibers underlying 
% that connectivity structure by Bézier curves. Bézier curves are defined
% by a set of control points $\mathbf{P}$:
%
% $$ \mathbf{B} = \mathbf{A}(\mathbf{t}) ~ \mathbf{P} + \epsilon,
% \qquad \epsilon \sim \mathcal{N}(\mathbf{0},\gamma \mathbf{I} ), $$
%
% where $\mathbf{t}$ is a vector of linearly spaced scalar values ranging from $0$ to $1$ and $\gamma$ 
% determines the spread of the fiber samples. For a cubic Bézier curve,
% matrix $\mathbf{A}$ is given by:
%
% $$ \mathbf{A}(\mathbf{t}) = (\mathbf{1} - \mathbf{t})^3, 3 (\mathbf{1} - \mathbf{t})^2 \mathbf{t}, 3(\mathbf{1}-\mathbf{t})\mathbf{t}^2, \mathbf{t}^3 ) $$. 
%
% Since Bézier curves are linear in the control points $\mathbf{P}$, we can determine the control points by multiplying 
% the left hand-side of equation \ref{eqn:bezier_curve} by the pseudo-inverse 
% $\mathbf{A}$ (i.e. $\mathbf{P} = \mathbf{A}^+ ( \mathbf{B} - \epsilon )$). Notice that the first 
% and last control points $\mathbf{p}_1,\mathbf{p}_4 \in \mathbf{P}$ mark the start and end of the Bézier curve, respectively. 
% We can therefore determine the connectivity scores by sampling from the marginal distribution of $\mathbf{p}_4$:
%
% $$ \mathbf{p}_4 \sim \mathcal{N}( (\mathbf{A}^+ \mathbf{B})_{(4,\cdot)},(\gamma \mathbf{A}^{+^T} \mathbf{A}^+)_{(4,\cdot)}) $$,
%
% We compare the posterior agreement to alternative validation criteria
% such as BIC and AIC.
%
% The performance of each validation criteria is summarized by the confusion matrices whose entries capture the frequency of estimated clusters (columns) 
% for a range of actual number of clusters (rows).

%% Input
nsamples = 300;
edges = [1:1:800];
K = 2:12;
nClusters = 2:7;
%%

% interpolate seed coord
seed_coords = importdata('seed_coords.mat'); seed_coords(:,2) = 500-seed_coords(:,2);
xyz = seed_coords';
[~,npts]=size(xyz);
xyzp=[];
for k=1:2
    xyzp(k,:)=fnval(csaps(1:npts,xyz(k,:)),[1:0.1:npts]);
end
tmp = randperm(size(xyzp,2)); tmp = tmp(1:200); tmp = sort(tmp);
seed_coords = xyzp(:,tmp)';

%% Learn spline control points
line = importdata('anatomy_lines.mat');
data = [];
for i = 1:length(line)
    line_obs{i} = line{i}; line_obs{i}(:,2) = 500-line_obs{i}(:,2);
    distribution{i} = learn_spline(line_obs{i});
    ctrl_pts(:,:,i) = distribution{i}.ctrl_pts.mean;
end

[ctrl_pts2,ctrl_pts] = rearrange_ctrl_pts(ctrl_pts);

seed_ctrl_pts = reshape(ctrl_pts(1,:,:),2,[])';

%%

% Initialize confusion matrix
confusion_matrix.PA = zeros(max(nClusters),max(K));
confusion_matrix.BIC = zeros(max(nClusters),max(K));
confusion_matrix.AIC = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence1 = zeros(max(nClusters),max(K));
confusion_matrix.bayes_evidence2 = zeros(max(nClusters),max(K));

%%

% start timer
tic;

%%
n_experiments =  1;
for n = 1:n_experiments
for  ktrue = nClusters
    clear fiber_assignment connectivity_matrix target_coord
    tmp0 = round(size(ctrl_pts2,3)*dirichletRnd(1,ones(1,ktrue)./0.1)); j=0;
    tmp0(end) = tmp0(end) + (size(ctrl_pts2,3) - sum(tmp0));
    for i = 1:ktrue
        for m = 1:tmp0(i)
            j=j+1;
            fiber_assignment{i}(m) = j;
        end
    end
    
    tmp = round(size(seed_coords,1)*dirichletRnd(1,ones(1,8)./0.1)); j=0; idx_start = 1; idx_end = 0;
    tmp(end) = tmp(end) + (size(seed_coords,1) - sum(tmp));
    seed_labels_true = ones(1,size(seed_coords,1));
    for i = 1:length(tmp)-1
        idx_start = idx_start+tmp(i);
        idx_end = idx_start+tmp(i+1)-1;
        seed_labels_true(idx_start:idx_end) = i+1;
    end
    
     %% Connectivity matrices
    
    h = setup_plots;
    colors = distinguishable_colors(length(fiber_assignment));
    
    for m = 1:2
        for i = 1:size(seed_coords,1)
            clear spline_sample
            for j = 1:nsamples
                label(i) = find(cellfun(@(x) ismember(seed_labels_true(i),x),fiber_assignment));
                prob = ones(1,length(fiber_assignment{label(i)}))./length(fiber_assignment{label(i)});
                sample_idx = discreternd(prob,1);
                sample_idx = fiber_assignment{label(i)}(sample_idx);
                seed_coord_ctrl_pts = ctrl_pts2(:,:,sample_idx);
                seed_coord_ctrl_pts(1,:) = seed_coords(i,:);
                spline_sample{j} = sample_spline(5e2,1e4,seed_coord_ctrl_pts,line_obs{sample_idx});
                target_coord{m}(j,:,i) = round(spline_sample{j}(end,:));
                
                s(i) = sample_idx;
            end
            if m==1
            hold on; p{i} = plot(h{1},spline_sample{j}(:,1),spline_sample{j}(:,2),'LineWidth',1);
            end
            connectivity_matrix{m}(i,:) = reshape(histcn(target_coord{m}(:,:,i),1:600,1:600),1,[]);
        end
    end
    
    for i = 1:length(p)
        p{i}.Color = colors(label(i),:);
    end
    scatter(h{1},seed_coords(:,1),seed_coords(:,2),50,colors(label,:),'filled')
    
    for i = 1:length(p)
        p2{i} = plot(h{2},p{i}.XData,p{i}.YData,'Color',[0.8,0.8,0.8],'LineWidth',1);
        hold on
    end
    scatter(h{2},seed_coords(:,1),seed_coords(:,2),50,[0,0,0],'filled')
    
    % remove zero columns
    siz = size(connectivity_matrix{1},2);
    rm_idx = sum(connectivity_matrix{1},1) + sum(connectivity_matrix{2},1) == 0;
    connectivity_matrix{1}(:,rm_idx) = [];
    connectivity_matrix{2}(:,rm_idx) = [];
    
    % normalize connectivity matrix
    % data{1} = sparse(bsxfun(@rdivide,connectivity_matrix{1},sum(connectivity_matrix{1},2)));
    % data{2} = sparse(bsxfun(@rdivide,connectivity_matrix{2},sum(connectivity_matrix{2},2)));
    data{1} = sparse(connectivity_matrix{1});
    data{2} = sparse(connectivity_matrix{2});
    
    % display connectivity matrix
    imagesc(h{3},flipdim(logical(connectivity_matrix{1}),1)); colormap(h{3},gray)
    h{3}.FontSize = 15; h{3}.XTick = []; h{3}.YTick = []; h{3}.Box = 'on';
    h{3}.Title.String = 'Connectivity matrix'; h{3}.XLabel.String = 'target voxels';
    h{3}.YLabel.String = 'seed voxels';
    
    % display dissimilarity matrix
    %dsim = flipdim(pdist2(data{1},data{2}),1);
    dsim = pdist2(data{1},data{2});
    imagesc(h{4},flipdim(flipdim(dsim,1),2));
    h{4}.FontSize = 15; h{4}.XTick = []; h{4}.YTick = []; h{4}.Box = 'on';
    h{4}.Title.String = 'Dissimilarity matrix';
    h{4}.XLabel.String = 'seed voxels'; h{4}.YLabel.String = 'seed voxels';
    
    %%
    
    % Bayesian evidence hyperparameters
    
    alpha{1} = full(sum(data{1},1));
    alpha{2} = full(sum(data{1},1))/100;
    
    %% Deterministic annealing
    % Determine global minimizer.
    
    % Annealing settings
    beta_init = 1e-4;     % starting inverse temperature
    beta_step = 1.1;      % inverse temperature step
    beta_stop = 1e-1;      % stopping inverse temperature
    perturb_sd = 1e-4;    % centroid perturbation
    
    % Initialization of information content
    info_content = zeros(1,max(K));
    
    for k = K
        % Initialization of Gibbs distributions
        gibbs_dist1 = ones(size(data{1},1),k) ./ k;
        gibbs_dist2 = ones(size(data{2},1),k) ./ k;
        
        % Initialization of centroids
        centroid1 = gibbs_dist1'*data{1};
        centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
        centroid1(centroid1==0) = eps;  % smoothing
        centroid2 = gibbs_dist2'*data{2};
        centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
        centroid2(centroid2==0) = eps;  % smoothing
        
        j = 0; beta = beta_init; inv_temp = []; %subplot(2,3,4); cla
        while beta <= beta_stop
            
            %%% Perturb centroids
            % Avoid local minimum by perturbing centroids:
            % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
            centroid1 = centroid1 + perturb_sd * rand(size(centroid1));
            centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2)); % normalize
            centroid2 = centroid2 + perturb_sd * rand(size(centroid2));
            centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2)); % normalize
            
            %% Expectation maximization
            % Iterate between determining Gibbs distributions and maximzing
            % variational lower bound w.r.t. centroids.
            for iter = 1:10
                
                %%% Costs for histogram clustering given instance 1
                % KL divergence between empirical probabilities (data) and
                % centroid probabilities (up to proportionality constant):
                % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
                potential1 = -data{1} * log(centroid1)';
                
                %%% Costs for histogram clustering given instance 2
                % KL divergence between empirical probabilities (data) and
                % centroid probabilities (up to proportionality constant):
                % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
                potential2 = -data{2} * log(centroid2)';
                
                %%% Gibbs distribution 1
                % Maximum entropy distribution:
                % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
                gibbs_dist1 = exp(-beta * potential1);
                partition_sum1 = sum(gibbs_dist1,2);
                gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
                
                % avoid underflow
                idx = find(partition_sum1==0);
                if ~isempty(idx)
                    [~,min_cost_idx] = min(potential1(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                    gibbs_dist1(idx,:) = zeros(length(idx),k);
                    gibbs_dist1(max_ind) = 1;
                end
                
                %%% Gibbs distribution 2
                % Maximum entropy distribution:
                % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
                gibbs_dist2 = exp(-beta * potential2);
                partition_sum2 = sum(gibbs_dist2,2);
                gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
                
                % avoid underflow
                idx = find(partition_sum2==0);
                if ~isempty(idx)
                    [~,min_cost_idx] = min(potential2(idx,:),[],2);
                    max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                    gibbs_dist2(idx,:) = zeros(length(idx),k);
                    gibbs_dist2(max_ind) = 1;
                end
                
                %%% Joint Gibbs distribution
                % Maximum entropy distribution:
                % $p_{ik}^{(1,2)} = \exp \left(-\beta (
                % R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$
                dist_joint = exp(-beta * (potential1 + potential2));
                joint_partition_sum = sum(dist_joint,2);
                
                %%% Centroids for instance 1
                % Probability prototype:
                % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
                centroid1 = gibbs_dist1'*data{1};
                centroid1 = bsxfun(@rdivide,centroid1,sum(centroid1,2));
                centroid1(centroid1==0) = eps;
                
                %%% Centroids for instance 2
                % Probability prototype:
                % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
                centroid2 = gibbs_dist2'*data{2};
                centroid2 = bsxfun(@rdivide,centroid2,sum(centroid2,2));
                centroid2(centroid2==0) = eps;
            end
            
            %%
            
            % increase inverse temperature:
            beta = beta * beta_step;
            
            %%% Match clusters across data instances
            % Use Hungarian algorithm to match clusters.
            match_clusters_idx = munkres(pdist2(centroid1,centroid2));
            potential2=potential2(:,match_clusters_idx);
            
            %%% Log partition sum for instance 1
            % Determine log partition sum while avoiding underflow:
            % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
            scaled_cost1 = -beta * potential1;
            % log-sum-exp trick to prevent underflow
            max_scaled_cost1 = max(scaled_cost1,[],2);
            log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
            
            %%% Log partition sum for instance 2
            % Determine log partition sum while avoiding underflow:
            % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
            scaled_cost2 = -beta * potential2;
            % log-sum-exp trick to prevent underflow
            max_scaled_cost2 = max(scaled_cost2,[],2);
            log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
            
            %%% Joint log partition sum
            % Determine joint log partition sum while avoiding underflow:
            % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
            joint_scaled_cost = -beta * (potential1 + potential2);
            gibbs_dist_joint = bsxfun(@rdivide,joint_scaled_cost,sum(joint_scaled_cost,2));
            % log-sum-exp trick to prevent underflow
            max_scaled_cost3 = max(joint_scaled_cost,[],2);
            log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
            
            %%
            
            j = j+1;
            
            %%% Generalization capacity
            % Resolution of the hypothesis space:
            % $GC(\beta) = \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
            gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 ...
                - log_partition_sum2) ./ size(partition_sum1,1);
            
            %%
            
            % pack
            inv_temp(j) = beta;
            gibbs_dist_packed1{k}(:,:,j) = gibbs_dist1;
            gibbs_dist_packed2{k}(:,:,j) = gibbs_dist2(:,match_clusters_idx);

        end
        
        %% Number of equivariant transformations
        % Richness of the hypothesis space:
        % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
        gibbs_dist1 = round(gibbs_dist1);
        d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
        nTransformations = -d * log(d)';
        
        % correct generalization capacity
        gc{k} = gc{k}-log(k)+nTransformations;
        
        gc{k} = gc{k} * log2(exp(1));  % transforming units from nats to bits
        
        %% Information content
        % Quality of algorithm:
        % $\mathcal{I} = \max_{\beta} GC(\beta)$
        [info_content(k),max_gc_idx(k)] = max(gc{k});
        
        %% Bayesian Information Criterion (BIC)
        % $BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the
        % number of bins and $c^{\perp}$ is the empirical risk minimizer
        
        cost = sum(sum(gibbs_dist1 .* (-data{1} * log(centroid1)')));
        BIC(k) = size(data{1},2) * k * log(size(data{1},1)) + 2 * cost;
        
        %% Akaike Information Criterion (AIC)
        % $AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the
        % number of bins and $c^{\perp}$ is the empirical risk minimizer
        
        AIC(k) = 2 * size(data{1},2) * k + 2 * cost;
        
        %% Bayesian evidence
        % $p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi)
        % p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k +
        % \mathbf{n}_k)}{B(\alpha_k)}$ where $\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$ 
        % and $n_{kj} := \sum_{i:c(i)=k} x_{ij}$
        
        [~,labels] = max(gibbs_dist1,[],2);
        log_bayes_evidence{1}(k) = 0;
        log_bayes_evidence{2}(k) = 0;
        for c = unique(labels)'
            ncj = full(sum(data{1}(labels==c,:),1));
            alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
            alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
            ncj(ncj==0) = [];
            
            log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) ...
                + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
            log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) ...
                + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
        end
    end
    
    %% Results
    [ktrue2,centroid_labels] = display_result(info_content,gc,inv_temp,BIC,AIC,log_bayes_evidence,...
        gibbs_dist_packed1,gibbs_dist_packed2,K,p2,h,seed_coords,data,fiber_assignment);

    %%% Update confusion matrices
    
    confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) = confusion_matrix.PA(ktrue2,length(unique(centroid_labels))) + 1;
    confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) = confusion_matrix.BIC(ktrue2,find(min(BIC(2:end))==BIC(2:end))+1) + 1;
    confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) = confusion_matrix.AIC(ktrue2,find(min(AIC(2:end))==AIC(2:end))+1) + 1;
    confusion_matrix.bayes_evidence1(ktrue2,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) = confusion_matrix.bayes_evidence1(ktrue,find(max(log_bayes_evidence{1}(2:end))==log_bayes_evidence{1}(2:end))+1) + 1;
    confusion_matrix.bayes_evidence2(ktrue2,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) = confusion_matrix.bayes_evidence2(ktrue,find(max(log_bayes_evidence{2}(2:end))==log_bayes_evidence{2}(2:end))+1) + 1;
    
    drawnow
end
end
%% Confusion Matrix
disp('Confusion Matrices'); disp(' ');
T.PA = array2table(confusion_matrix.PA(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Posterior Agreement:'); disp(T.PA);
T.BIC = array2table(confusion_matrix.BIC(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('BIC:'); disp(T.BIC);
T.AIC = array2table(confusion_matrix.AIC(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('AIC:'); disp(T.AIC);
T.bayes_evidence1 = array2table(confusion_matrix.bayes_evidence1(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Bayes_evidence 1:'); disp(T.bayes_evidence1);
T.bayes_evidence2 = array2table(confusion_matrix.bayes_evidence2(2:end,2:end),'VariableNames',{'k2','k3','k4','k5','k6','k7','k8','k9','k10','k11','k12'},'RowNames',{'k2','k3','k4','k5','k6','k7'});
disp('Bayes_evidence 2:'); disp(T.bayes_evidence2);
%%

% display runtime
disp(['runtime = ' num2str(toc/60) ' minutes']);

##### SOURCE END #####
--></body></html>


<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>POSTERIOR AGREEMENT FOR CONNECTIVITY-BASED CORTEX PARCELLATION</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-11-07"><meta name="DC.source" content="cortex_parcellation.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>POSTERIOR AGREEMENT FOR CONNECTIVITY-BASED CORTEX PARCELLATION</h1><!--introduction--><p><b>Author: Nico Stephan Gorbach</b> , Institute of Machine Learning, ETHZ, email: <a href="mailto:nico.gorbach@inf.ethz.ch">nico.gorbach@inf.ethz.ch</a></p><p>Implementation of " <b>Pipeline Validation for Connectivity-based Cortex Parcellation</b> " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#3">Input</a></li><li><a href="#4">Imports</a></li><li><a href="#5">Distance between connectivity matrices</a></li><li><a href="#6">Nearest neighbours for smooth histogram clustering</a></li><li><a href="#11">Deterministic annealing</a></li><li><a href="#14">Perturb centroids</a></li><li><a href="#15">Expectation maximization</a></li><li><a href="#16">Costs for histogram clustering given instance 1</a></li><li><a href="#17">Costs for histogram clustering given instance 2</a></li><li><a href="#18">Smoothness potential</a></li><li><a href="#19">Gibbs distribution 1</a></li><li><a href="#20">Gibbs distribution 2</a></li><li><a href="#21">Joint Gibbs distribution</a></li><li><a href="#22">Centroids for instance 1</a></li><li><a href="#23">Centroids for instance 2</a></li><li><a href="#25">Match clusters across data instances</a></li><li><a href="#26">Log partition sum for instance 1</a></li><li><a href="#27">Log partition sum for instance 2</a></li><li><a href="#28">Joint log partition sum</a></li><li><a href="#30">Generalization capacity</a></li><li><a href="#33">Number of equivariant transformations</a></li><li><a href="#35">Information content (ASC)</a></li><li><a href="#36">Bayesian Information Criterion (BIC)</a></li><li><a href="#37">Akaike Information Criterion (AIC)</a></li><li><a href="#38">Bayesian evidence</a></li><li><a href="#39">Number of misclustered seed voxels</a></li><li><a href="#40">Distance between centroids</a></li><li><a href="#41">Results</a></li><li><a href="#43">Runtime</a></li></ul></div><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>
</pre><h2 id="2">Introduction</h2><p>This instuctional code demonstrates validation by posterior agreement for connectivity-based cortex parcellation of diffusion weighted imaging data. We assume that a connectivity matrix is given. In particular, we validate histogram clustering. Additionally, we provide the option of validating smooth histogram clustering (by setting smooth_clustering=true in the input) which adds a spatial regularity to the seed voxels.</p><h2 id="3">Input</h2><pre class="codeinput">path.connectivity_matrix = <span class="string">'./5203/neighbourhood_tracking/45_directions/connectivity_matrix.mat'</span>;    <span class="comment">% path to connectivity matrix</span>
path.seed_coords = <span class="string">'./5203/neighbourhood_tracking/45_directions/seed_coords.txt'</span>;                    <span class="comment">% path to seed coordinates</span>
path.results_directory = <span class="string">'./5203/neighbourhood_tracking/45_directions/'</span>;                             <span class="comment">% directory to save results</span>
K = 2:3:29;                                                     <span class="comment">% number of potential clusters (vector of integers)</span>

<span class="comment">% Smooth histogram clustering</span>
smooth_clustering = false;                                    <span class="comment">% perform standard histogram clustering or smooth histogram clustering (Boolean)</span>
smooth_weighting = 10;                                        <span class="comment">% weight of the smoothness penalty (real positive number)</span>
number_of_neighbours = 80;                                    <span class="comment">% number of neighbours %20,50,80</span>
</pre><h2 id="4">Imports</h2><pre class="codeinput">connectivity_matrix = importdata(path.connectivity_matrix);
seed_coords = importdata(path.seed_coords);
</pre><h2 id="5">Distance between connectivity matrices</h2><pre class="codeinput">seed_idx = randperm(size(connectivity_matrix{1},1)); seed_idx = sort(seed_idx(1:1000));
p = bsxfun(@rdivide,connectivity_matrix{1}(seed_idx,:),sum(connectivity_matrix{1}(seed_idx,:),2));
q = bsxfun(@rdivide,connectivity_matrix{2}(seed_idx,:),sum(connectivity_matrix{2}(seed_idx,:),2));
<span class="keyword">for</span> i = 1:size(p,1)
    dsim_across_instances.JSDiv(i) = JSDiv(p(i,:),q(i,:));
    dsim_across_instances.euclid(i) = pdist2(p(i,:),q(i,:));
    dsim_across_instances.hamming = pdist2(double(logical(p(i,:))),double(logical(q(i,:))),<span class="string">'hamming'</span>);
<span class="keyword">end</span>

disp([<span class="string">'Average Jenson Shannon distance between connectivity matrices: '</span> num2str(mean(dsim_across_instances.JSDiv))]);
disp([<span class="string">'Average Euclidean distance between connectivity matrices: '</span> num2str(mean(dsim_across_instances.euclid))]);
disp([<span class="string">'Average Hamming distance between connectivity matrices: '</span> num2str(mean(dsim_across_instances.hamming))]);
</pre><pre class="codeoutput">Average Jenson Shannon distance between connectivity matrices: 0.54389
Average Euclidean distance between connectivity matrices: 0.065424
Average Hamming distance between connectivity matrices: 0.012757
</pre><h2 id="6">Nearest neighbours for smooth histogram clustering</h2><pre class="codeinput"><span class="keyword">if</span> smooth_clustering
    dsim_seed_coords = pdist2(seed_coords,seed_coords);
    dsim_seed_coords(logical(eye(size(dsim_seed_coords,1)))) = realmax;
    [nearest_neighbours.dist,nearest_neighbours.idx] = sort(dsim_seed_coords,2,<span class="string">'ascend'</span>);
    nearest_neighbours.dist = nearest_neighbours.dist(:,1:number_of_neighbours);
    nearest_neighbours.idx = nearest_neighbours.idx(:,1:number_of_neighbours);
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="comment">% remove zero columns</span>
rm_idx(1,:) = sum(connectivity_matrix{1},1)==0;
rm_idx(2,:) = sum(connectivity_matrix{2},1)==0;
connectivity_matrix{1}(:,rm_idx(1,:)) = [];
connectivity_matrix{2}(:,rm_idx(2,:)) = [];
</pre><pre class="codeinput"><span class="comment">% dissimilarity matrix</span>
<span class="keyword">if</span> exist([path.results_directory <span class="string">'dsim.mat1'</span>])
    dsim = importdata([path.results_directory <span class="string">'dsim.mat'</span>]);
<span class="keyword">else</span>
    <span class="comment">%d = connectivity_matrix{1}; d(:,sum(d,1)&lt;4000) = [];</span>
    d = connectivity_matrix{1}; d(:,sum(d,1)&lt;600) = []; d = single(full(d));
    dsim = pdist2(d,d);
<span class="comment">%     d = bsxfun(@rdivide,d,sum(d,2));</span>
<span class="comment">%     log_d = d; log_d(log_d==0) = eps; log_d = log(log_d);</span>
<span class="comment">%     dsim = single(full(-d * log_d'));</span>
<span class="comment">%     dsim = flipdim(flipdim((dsim + dsim') / 2,1),2);</span>
<span class="comment">%     dsim(logical(eye(size(dsim,1)))) = mean(mean(dsim));</span>
<span class="comment">%     save([path.results_directory 'dsim.mat'],'dsim','-v7.3');</span>
<span class="comment">%     clear d log_d</span>
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="comment">% preprocessing for Bayesian evidence</span>

alpha{1} = full(sum(connectivity_matrix{1},1));
alpha{2} = full(sum(connectivity_matrix{1},1))/100;
</pre><pre class="codeinput"><span class="comment">% start timer</span>
tic;
</pre><h2 id="11">Deterministic annealing</h2><p>Determine global minimizer.</p><pre class="codeinput"><span class="comment">% Annealing settings</span>
inv_temp_init = 1/1000;                 <span class="comment">% starting inverse temperature (5000 for smooth hc)</span>
inv_temp_step = 1.1;                    <span class="comment">% inverse temperature step</span>
inv_temp_stop = 1/10;                   <span class="comment">% stopping inverse temperature</span>
perturb_sd = 1e-6;                      <span class="comment">% centroid perturbation</span>

<span class="keyword">for</span> k = K
</pre><pre class="codeinput">    <span class="comment">% Initialization of Gibbs distributions</span>
    gibbs_dist1 = ones(size(connectivity_matrix{1},1),k) ./ k;
    gibbs_dist2 = ones(size(connectivity_matrix{2},1),k) ./ k;

    <span class="comment">% Initialization of centroids</span>
    centroids1 = gibbs_dist1'*connectivity_matrix{1};
    centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2));
    centroids1(centroids1==0) = eps;
    centroids2 = gibbs_dist2'*connectivity_matrix{2};
    centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2));
    centroids2(centroids2==0) = eps;

    j = 0; inv_temp = inv_temp_init; inv_temp_pack = []; <span class="comment">%subplot(2,2,3); cla</span>
    <span class="keyword">while</span> inv_temp &lt;= inv_temp_stop
</pre><h2 id="14">Perturb centroids</h2><p>Avoid local minimum by perturbing centroids: <img src="cortex_parcellation_eq00657792805889241601.png" alt="$\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$" style="width:63px;height:17px;"></p><pre class="codeinput">        centroids1 = centroids1 + perturb_sd * rand(size(centroids1));
        centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2)); <span class="comment">% normalize</span>
        centroids2 = centroids2 + perturb_sd * rand(size(centroids2));
        centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2)); <span class="comment">% normalize</span>
</pre><h2 id="15">Expectation maximization</h2><p>Iterate between determining Gibbs distributions and maximzing variational lower bound w.r.t. centroids.</p><pre class="codeinput">        <span class="keyword">for</span> iter = 1:10
</pre><h2 id="16">Costs for histogram clustering given instance 1</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="cortex_parcellation_eq08373992262661706917.png" alt="$R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential1 = -connectivity_matrix{1} * log(centroids1)';
</pre><h2 id="17">Costs for histogram clustering given instance 2</h2><p>KL divergence between empirical probabilities (data) and centroid probabilities (up to proportionality constant): <img src="cortex_parcellation_eq14470087101677573895.png" alt="$R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$" style="width:119px;height:17px;"></p><pre class="codeinput">            potential2 = -connectivity_matrix{2} * log(centroids2)';
</pre><h2 id="18">Smoothness potential</h2><pre class="codeinput">            <span class="keyword">if</span> smooth_clustering
                smooth_potential1 = blockfun(1-gibbs_dist1(nearest_neighbours.idx',:),<span class="keyword">...</span>
                    [size(nearest_neighbours.idx,2),1],@sum);

                smooth_potential2 = blockfun(1-gibbs_dist2(nearest_neighbours.idx',:),<span class="keyword">...</span>
                    [size(nearest_neighbours.idx,2),1],@sum);

                potential1 = potential1 + smooth_weighting * smooth_potential1;
                potential2 = potential2 + smooth_weighting * smooth_potential2;
            <span class="keyword">end</span>
</pre><h2 id="19">Gibbs distribution 1</h2><p>Maximum entropy distribution: <img src="cortex_parcellation_eq04115272212719140819.png" alt="$p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist1 = exp(-inv_temp * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum1==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);
                gibbs_dist1(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="20">Gibbs distribution 2</h2><p>Maximum entropy distribution: <img src="cortex_parcellation_eq17712409327870511715.png" alt="$p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$" style="width:112px;height:20px;"></p><pre class="codeinput">            gibbs_dist2 = exp(-inv_temp * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);

            <span class="comment">% avoid underflow</span>
            idx = find(partition_sum2==0);
            <span class="keyword">if</span> ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2);
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);
                gibbs_dist2(max_ind) = 1;
            <span class="keyword">end</span>
</pre><h2 id="21">Joint Gibbs distribution</h2><p>Maximum entropy distribution: <img src="cortex_parcellation_eq01026291022421135752.png" alt="$p_{ik}^{(1,2)} = \exp \left(-\beta ( R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$" style="width:159px;height:20px;"></p><pre class="codeinput">            dist_joint = exp(-inv_temp * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
</pre><h2 id="22">Centroids for instance 1</h2><p>Probability prototype: <img src="cortex_parcellation_eq09863515348217012097.png" alt="$\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroids1 = gibbs_dist1'*connectivity_matrix{1};
            centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2));
            centroids1(centroids1==0) = eps;
</pre><h2 id="23">Centroids for instance 2</h2><p>Probability prototype: <img src="cortex_parcellation_eq00293485138018580756.png" alt="$\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$" style="width:90px;height:27px;"></p><pre class="codeinput">            centroids2 = gibbs_dist2'*connectivity_matrix{2};
            centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2));
            centroids2(centroids2==0) = eps;
        <span class="keyword">end</span>
</pre><pre class="codeinput">        <span class="comment">% increase inverse temperature:</span>
        inv_temp = inv_temp * inv_temp_step;
</pre><h2 id="25">Match clusters across data instances</h2><p>Use Hungarian algorithm to match clusters.</p><pre class="codeinput">        c1 = zeros(k,length(rm_idx(1,:))); c2 = zeros(k,length(rm_idx(2,:)));
        c1(:,~rm_idx(1,:)) = centroids1; c2(:,~rm_idx(2,:)) = centroids2;
        match_clusters_idx = munkres(pdist2(c1,c2));
        potential2=potential2(:,match_clusters_idx);
</pre><h2 id="26">Log partition sum for instance 1</h2><p>Determine log partition sum while avoiding underflow: <img src="cortex_parcellation_eq16546713842241333105.png" alt="$\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost1 = -inv_temp * potential1;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
</pre><h2 id="27">Log partition sum for instance 2</h2><p>Determine log partition sum while avoiding underflow: <img src="cortex_parcellation_eq15044720363000448952.png" alt="$\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$" style="width:158px;height:20px;"></p><pre class="codeinput">        scaled_cost2 = -inv_temp * potential2;
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
</pre><h2 id="28">Joint log partition sum</h2><p>Determine joint log partition sum while avoiding underflow: <img src="cortex_parcellation_eq14109611573114866887.png" alt="$\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$" style="width:202px;height:20px;"></p><pre class="codeinput">        joint_scaled_cost = -inv_temp * (potential1 + potential2);
        <span class="comment">% log-sum-exp trick to prevent underflow</span>
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
</pre><pre class="codeinput">        j = j+1;
</pre><h2 id="30">Generalization capacity</h2><p>Resolution of the hypothesis space: <img src="cortex_parcellation_eq00804782376720270041.png" alt="$GC(\beta) := \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$" style="width:224px;height:14px;"></p><pre class="codeinput">        gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 <span class="keyword">...</span>
            - log_partition_sum2) ./ size(partition_sum1,1);
</pre><pre class="codeinput">        <span class="comment">% pack</span>
        inv_temp_pack(j) = inv_temp;
        gibbs_dist_packed1{k}(:,:,j) = single(gibbs_dist1);
        gibbs_dist_packed2{k}(:,:,j) = single(gibbs_dist2(:,match_clusters_idx));

<span class="comment">%         gc_plot = gc{k}; %gc_plot(gc_plot&lt;0) = NaN;</span>
<span class="comment">%         hold on; plot(inv_temp,gc_plot,'LineWidth',2); drawnow</span>
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><h2 id="33">Number of equivariant transformations</h2><p>Richness of the hypothesis space: <img src="cortex_parcellation_eq11001603530755112468.png" alt="$\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$" style="width:151px;height:14px;"></p><pre class="codeinput">    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';

    <span class="comment">% correct generalization capacity</span>
    gc{k} = gc{k}-log(k)+nTransformations;
</pre><pre class="codeinput">    <span class="comment">% transforming units from nats to bits</span>
    gc{k} = gc{k} * log2(exp(1));
</pre><h2 id="35">Information content (ASC)</h2><p>Quality of algorithm: <img src="cortex_parcellation_eq09743455593975302450.png" alt="$\mathcal{I} := \max_{\beta} GC(\beta)$" style="width:84px;height:12px;"></p><pre class="codeinput">    [info_content(k),max_gc_idx(k)] = max(gc{k});
</pre><h2 id="36">Bayesian Information Criterion (BIC)</h2><p><img src="cortex_parcellation_eq09700827967035327629.png" alt="$BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$" style="width:164px;height:14px;"> where <img src="cortex_parcellation_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="cortex_parcellation_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">    <span class="keyword">if</span> ~smooth_clustering
        cost = sum(sum(gibbs_dist1 .* (-connectivity_matrix{1} * log(centroids1)')));
    <span class="keyword">else</span>
        cost = sum(sum(potential1(logical(gibbs_dist1))));
    <span class="keyword">end</span>
    BIC(k) = size(connectivity_matrix{1},2) * k * log(size(connectivity_matrix{1},1)) + 2 * cost;
</pre><h2 id="37">Akaike Information Criterion (AIC)</h2><p><img src="cortex_parcellation_eq03357731097339886371.png" alt="$AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$" style="width:143px;height:14px;"> where <img src="cortex_parcellation_eq11319871188381094158.png" alt="$m$" style="width:10px;height:6px;"> is the number of bins and <img src="cortex_parcellation_eq17108415063982397339.png" alt="$c^{\perp}$" style="width:11px;height:10px;"> is the empirical risk minimizer</p><pre class="codeinput">    AIC(k) = 2 * size(connectivity_matrix{1},2) * k + 2 * cost;
</pre><h2 id="38">Bayesian evidence</h2><p><img src="cortex_parcellation_eq08403349875303284569.png" alt="$p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi) p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k + \mathbf{n}_k)}{B(\alpha_k)}$" style="width:220px;height:18px;"> where <img src="cortex_parcellation_eq08059537060765087149.png" alt="$\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$" style="width:113px;height:11px;"> and <img src="cortex_parcellation_eq07727016875268873876.png" alt="$n_{kj} := \sum_{i:c(i)=k} x_{ij}$" style="width:86px;height:14px;"></p><pre class="codeinput">    [~,labels] = max(gibbs_dist1,[],2);
    log_bayes_evidence{1}(k) = 0;
    log_bayes_evidence{2}(k) = 0;
    <span class="keyword">for</span> c = unique(labels)'
        ncj = full(sum(connectivity_matrix{1}(labels==c,:),1));
        alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
        alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
        ncj(ncj==0) = [];

        log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) <span class="keyword">...</span>
            + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
        log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) <span class="keyword">...</span>
            + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
    <span class="keyword">end</span>
</pre><h2 id="39">Number of misclustered seed voxels</h2><pre class="codeinput">    number_misclustered_objects(k) = sum(diag(round(gibbs_dist1)' * ~round(gibbs_dist2(:,match_clusters_idx))))/2;
</pre><h2 id="40">Distance between centroids</h2><pre class="codeinput">    [~,max_gc_idx] = max(gc{k});
    centroids_opt = double(gibbs_dist_packed1{k}(:,:,max_gc_idx))'*connectivity_matrix{1};
    centroids_opt = bsxfun(@rdivide,centroids_opt,sum(centroids_opt,2));
    centroids_opt(centroids_opt==0) = eps;
</pre><h2 id="41">Results</h2><pre class="codeinput">    display_result(gc,info_content,gibbs_dist_packed1,gibbs_dist_packed2,inv_temp_pack,log_bayes_evidence,BIC,AIC,K(1:find(K==k)),<span class="keyword">...</span>
        dsim,centroids_opt,number_misclustered_objects,seed_coords);
</pre><pre class="codeoutput">Number of potential clusters: 2
</pre><img vspace="5" hspace="5" src="cortex_parcellation_01.png" style="width:1680px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 5
</pre><img vspace="5" hspace="5" src="cortex_parcellation_02.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 8
</pre><img vspace="5" hspace="5" src="cortex_parcellation_03.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 11
</pre><img vspace="5" hspace="5" src="cortex_parcellation_04.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 14
</pre><img vspace="5" hspace="5" src="cortex_parcellation_05.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 17
</pre><img vspace="5" hspace="5" src="cortex_parcellation_06.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 20
</pre><img vspace="5" hspace="5" src="cortex_parcellation_07.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 23
</pre><img vspace="5" hspace="5" src="cortex_parcellation_08.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 26
</pre><img vspace="5" hspace="5" src="cortex_parcellation_09.png" style="width:1700px;height:950px;" alt=""> <pre class="codeoutput">Number of potential clusters: 29
</pre><img vspace="5" hspace="5" src="cortex_parcellation_10.png" style="width:1700px;height:950px;" alt=""> <pre class="codeinput"><span class="keyword">end</span>
</pre><h2 id="43">Runtime</h2><pre class="codeinput"><span class="comment">% display runtime</span>
disp([<span class="string">'runtime: '</span> num2str(toc/60) <span class="string">' minutes on '</span> computer])
</pre><pre class="codeoutput">runtime: 30.2557 minutes on MACI64
</pre><pre class="codeinput"><span class="comment">% save resutls</span>
<span class="comment">%save([path.results_directory 'parcellation.mat'],'gibbs_dist_packed1','gibbs_dist_packed2','gc','info_content','BIC','AIC','log_bayes_evidence','dsim_across_instances')</span>
save([path.results_directory <span class="string">'parcellation.mat'</span>],<span class="string">'gibbs_dist_packed1'</span>,<span class="string">'gibbs_dist_packed2'</span>,<span class="string">'gc'</span>,<span class="string">'info_content'</span>,<span class="string">'BIC'</span>,<span class="string">'AIC'</span>,<span class="string">'log_bayes_evidence'</span>)
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% POSTERIOR AGREEMENT FOR CONNECTIVITY-BASED CORTEX PARCELLATION
% *Author: Nico Stephan Gorbach* ,
% Institute of Machine Learning, ETHZ,
% email: nico.gorbach@inf.ethz.ch
%
% Implementation of " *Pipeline Validation for Connectivity-based Cortex Parcellation* 
% " by Nico S. Gorbach, Marc Tittgemeyer and Joachim M. Buhmann 
%

%%
clear all; close all

%% Introduction
% This instuctional code demonstrates validation by posterior agreement for connectivity-based cortex
% parcellation of diffusion weighted imaging data. We assume that a
% connectivity matrix is given. In particular, we validate histogram
% clustering. Additionally, we provide the option of validating smooth
% histogram clustering (by setting smooth_clustering=true in the input) which 
% adds a spatial regularity to the seed voxels.
  
%% Input
path.connectivity_matrix = './5203/neighbourhood_tracking/45_directions/connectivity_matrix.mat';    % path to connectivity matrix
path.seed_coords = './5203/neighbourhood_tracking/45_directions/seed_coords.txt';                    % path to seed coordinates
path.results_directory = './5203/neighbourhood_tracking/45_directions/';                             % directory to save results                                        
K = 2:3:29;                                                     % number of potential clusters (vector of integers)

% Smooth histogram clustering
smooth_clustering = false;                                    % perform standard histogram clustering or smooth histogram clustering (Boolean)  
smooth_weighting = 10;                                        % weight of the smoothness penalty (real positive number) 
number_of_neighbours = 80;                                    % number of neighbours %20,50,80

%% Imports
connectivity_matrix = importdata(path.connectivity_matrix);
seed_coords = importdata(path.seed_coords);

%% Distance between connectivity matrices
seed_idx = randperm(size(connectivity_matrix{1},1)); seed_idx = sort(seed_idx(1:1000));
p = bsxfun(@rdivide,connectivity_matrix{1}(seed_idx,:),sum(connectivity_matrix{1}(seed_idx,:),2));
q = bsxfun(@rdivide,connectivity_matrix{2}(seed_idx,:),sum(connectivity_matrix{2}(seed_idx,:),2));
for i = 1:size(p,1)
    dsim_across_instances.JSDiv(i) = JSDiv(p(i,:),q(i,:));
    dsim_across_instances.euclid(i) = pdist2(p(i,:),q(i,:));
    dsim_across_instances.hamming = pdist2(double(logical(p(i,:))),double(logical(q(i,:))),'hamming');
end

disp(['Average Jenson Shannon distance between connectivity matrices: ' num2str(mean(dsim_across_instances.JSDiv))]);
disp(['Average Euclidean distance between connectivity matrices: ' num2str(mean(dsim_across_instances.euclid))]);
disp(['Average Hamming distance between connectivity matrices: ' num2str(mean(dsim_across_instances.hamming))]);

%% Nearest neighbours for smooth histogram clustering
if smooth_clustering
    dsim_seed_coords = pdist2(seed_coords,seed_coords);
    dsim_seed_coords(logical(eye(size(dsim_seed_coords,1)))) = realmax;
    [nearest_neighbours.dist,nearest_neighbours.idx] = sort(dsim_seed_coords,2,'ascend');
    nearest_neighbours.dist = nearest_neighbours.dist(:,1:number_of_neighbours);
    nearest_neighbours.idx = nearest_neighbours.idx(:,1:number_of_neighbours);
end

%%

% remove zero columns
rm_idx(1,:) = sum(connectivity_matrix{1},1)==0;
rm_idx(2,:) = sum(connectivity_matrix{2},1)==0;
connectivity_matrix{1}(:,rm_idx(1,:)) = [];
connectivity_matrix{2}(:,rm_idx(2,:)) = [];

%%

% dissimilarity matrix
if exist([path.results_directory 'dsim.mat1'])
    dsim = importdata([path.results_directory 'dsim.mat']);
else
    %d = connectivity_matrix{1}; d(:,sum(d,1)<4000) = [];
    d = connectivity_matrix{1}; d(:,sum(d,1)<600) = []; d = single(full(d));
    dsim = pdist2(d,d);
%     d = bsxfun(@rdivide,d,sum(d,2));
%     log_d = d; log_d(log_d==0) = eps; log_d = log(log_d);
%     dsim = single(full(-d * log_d'));
%     dsim = flipdim(flipdim((dsim + dsim') / 2,1),2);
%     dsim(logical(eye(size(dsim,1)))) = mean(mean(dsim));
%     save([path.results_directory 'dsim.mat'],'dsim','-v7.3');
%     clear d log_d
end
%%

% preprocessing for Bayesian evidence

alpha{1} = full(sum(connectivity_matrix{1},1));
alpha{2} = full(sum(connectivity_matrix{1},1))/100;
%%

% start timer
tic;

%% Deterministic annealing
% Determine global minimizer.

% Annealing settings
inv_temp_init = 1/1000;                 % starting inverse temperature (5000 for smooth hc)
inv_temp_step = 1.1;                    % inverse temperature step
inv_temp_stop = 1/10;                   % stopping inverse temperature
perturb_sd = 1e-6;                      % centroid perturbation

for k = K
    
    % Initialization of Gibbs distributions
    gibbs_dist1 = ones(size(connectivity_matrix{1},1),k) ./ k;
    gibbs_dist2 = ones(size(connectivity_matrix{2},1),k) ./ k;
    
    % Initialization of centroids
    centroids1 = gibbs_dist1'*connectivity_matrix{1};
    centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2)); 
    centroids1(centroids1==0) = eps;
    centroids2 = gibbs_dist2'*connectivity_matrix{2};
    centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2)); 
    centroids2(centroids2==0) = eps;
    
    j = 0; inv_temp = inv_temp_init; inv_temp_pack = []; %subplot(2,2,3); cla
    while inv_temp <= inv_temp_stop
        
        %%% Perturb centroids
        % Avoid local minimum by perturbing centroids:
        % $\phi_{kj}^{(\cdot)} = \phi_{kj}^{(\cdot)} + \epsilon$
        centroids1 = centroids1 + perturb_sd * rand(size(centroids1)); 
        centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2)); % normalize
        centroids2 = centroids2 + perturb_sd * rand(size(centroids2)); 
        centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2)); % normalize

        %% Expectation maximization
        % Iterate between determining Gibbs distributions and maximzing
        % variational lower bound w.r.t. centroids.
        for iter = 1:10
         
            %%% Costs for histogram clustering given instance 1
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(1)} = -\sum_{j} x_{ij}^{(1)} \log(\phi_{kj}^{(1)})$
            potential1 = -connectivity_matrix{1} * log(centroids1)';
            
            %%% Costs for histogram clustering given instance 2
            % KL divergence between empirical probabilities (data) and
            % centroid probabilities (up to proportionality constant):
            % $R_{ik}^{(2)} = -\sum_{j} x_{ij}^{(2)} \log(\phi_{kj}^{(2)})$
            potential2 = -connectivity_matrix{2} * log(centroids2)'; 
           
            %%% Smoothness potential
            if smooth_clustering
                smooth_potential1 = blockfun(1-gibbs_dist1(nearest_neighbours.idx',:),...
                    [size(nearest_neighbours.idx,2),1],@sum);
                
                smooth_potential2 = blockfun(1-gibbs_dist2(nearest_neighbours.idx',:),...
                    [size(nearest_neighbours.idx,2),1],@sum);
               
                potential1 = potential1 + smooth_weighting * smooth_potential1;
                potential2 = potential2 + smooth_weighting * smooth_potential2;
            end
            
            %%% Gibbs distribution 1
            % Maximum entropy distribution:
            % $p_{ik}^{(1)} = \exp \left( -\beta R_{ik}^{(1)} \right) / Z$
            gibbs_dist1 = exp(-inv_temp * potential1);
            partition_sum1 = sum(gibbs_dist1,2);
            gibbs_dist1 = bsxfun(@rdivide,gibbs_dist1,partition_sum1);
            
            % avoid underflow
            idx = find(partition_sum1==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential1(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist1),idx,min_cost_idx);
                gibbs_dist1(idx,:) = zeros(length(idx),k);  
                gibbs_dist1(max_ind) = 1;
            end

            %%% Gibbs distribution 2
            % Maximum entropy distribution:
            % $p_{ik}^{(2)} = \exp \left(-\beta R_{ik}^{(2)} \right) / Z$
            gibbs_dist2 = exp(-inv_temp * potential2);
            partition_sum2 = sum(gibbs_dist2,2);
            gibbs_dist2 = bsxfun(@rdivide,gibbs_dist2,partition_sum2);
            
            % avoid underflow
            idx = find(partition_sum2==0);
            if ~isempty(idx)
                [~,min_cost_idx] = min(potential2(idx,:),[],2); 
                max_ind = sub2ind(size(gibbs_dist2),idx,min_cost_idx);
                gibbs_dist2(idx,:) = zeros(length(idx),k);  
                gibbs_dist2(max_ind) = 1;
            end

            %%% Joint Gibbs distribution
            % Maximum entropy distribution:
            % $p_{ik}^{(1,2)} = \exp \left(-\beta (
            % R_{ik}^{(1)} + R_{ik}^{(2)}) \right) / Z$
            dist_joint = exp(-inv_temp * (potential1 + potential2));
            joint_partition_sum = sum(dist_joint,2);
            
            %%% Centroids for instance 1
            % Probability prototype:
            % $\phi_{kj}^{(1)} = \frac{\sum_i p_{ik}^{(1)} x_{ij}^{(1)}}{\sum_j \sum_{i'} p_{i'k}^{(1)} x_{i'j}^{(1)}}$
            centroids1 = gibbs_dist1'*connectivity_matrix{1};
            centroids1 = bsxfun(@rdivide,centroids1,sum(centroids1,2)); 
            centroids1(centroids1==0) = eps;
            
            %%% Centroids for instance 2
            % Probability prototype:
            % $\phi_{kj}^{(2)} = \frac{\sum_i p_{ik}^{(2)} x_{ij}^{(2)}}{\sum_j \sum_{i'} p_{i'k}^{(2)} x_{i'j}^{(2)}}$
            centroids2 = gibbs_dist2'*connectivity_matrix{2};
            centroids2 = bsxfun(@rdivide,centroids2,sum(centroids2,2)); 
            centroids2(centroids2==0) = eps;     
        end
        
        %%
        
        % increase inverse temperature:
        inv_temp = inv_temp * inv_temp_step;

        %%% Match clusters across data instances
        % Use Hungarian algorithm to match clusters.
        c1 = zeros(k,length(rm_idx(1,:))); c2 = zeros(k,length(rm_idx(2,:)));
        c1(:,~rm_idx(1,:)) = centroids1; c2(:,~rm_idx(2,:)) = centroids2;
        match_clusters_idx = munkres(pdist2(c1,c2));
        potential2=potential2(:,match_clusters_idx);
        
        %%% Log partition sum for instance 1
        % Determine log partition sum while avoiding underflow:
        % $\log Z_1 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(1)} \right)$
        scaled_cost1 = -inv_temp * potential1;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost1 = max(scaled_cost1,[],2);
        log_partition_sum1 = max_scaled_cost1 + log(sum(exp(scaled_cost1-max_scaled_cost1),2));
        
        %%% Log partition sum for instance 2
         % Determine log partition sum while avoiding underflow:
        % $\log Z_2 = \sum_i \log \sum_k \exp \left( -\beta R_{ik}^{(2)} \right)$
        scaled_cost2 = -inv_temp * potential2;
        % log-sum-exp trick to prevent underflow
        max_scaled_cost2 = max(scaled_cost2,[],2);
        log_partition_sum2 = max_scaled_cost2 + log(sum(exp(scaled_cost2-max_scaled_cost2),2));
        
        %%% Joint log partition sum
        % Determine joint log partition sum while avoiding underflow:
        % $\log Z_{12} = \sum_i \log \sum_k \exp \left( -\beta ( R_{ik}^{(1)} + R_{ik}^{(2)} )  \right)$
        joint_scaled_cost = -inv_temp * (potential1 + potential2);
        % log-sum-exp trick to prevent underflow
        max_scaled_cost3 = max(joint_scaled_cost,[],2);
        log_joint_partition_sum = max_scaled_cost3 + log(sum(exp(joint_scaled_cost-max_scaled_cost3),2));
       
        %%
        
        j = j+1;
        
        %%% Generalization capacity
        % Resolution of the hypothesis space:
        % $GC(\beta) := \log(k) + \frac{1}{n} \left( \log Z_{12} - \log Z_1 - \log Z_2 \right)$
        gc{k}(j) = log(k) + sum(log_joint_partition_sum - log_partition_sum1 ...
            - log_partition_sum2) ./ size(partition_sum1,1);
        
        %%
        
        % pack
        inv_temp_pack(j) = inv_temp;
        gibbs_dist_packed1{k}(:,:,j) = single(gibbs_dist1);
        gibbs_dist_packed2{k}(:,:,j) = single(gibbs_dist2(:,match_clusters_idx));
        
%         gc_plot = gc{k}; %gc_plot(gc_plot<0) = NaN;
%         hold on; plot(inv_temp,gc_plot,'LineWidth',2); drawnow

    end

    %% Number of equivariant transformations
    % Richness of the hypothesis space: 
    % $\frac{1}{n} \log|\{\tau\}| = H(n_1/n,\ldots,n_k/n)$
    gibbs_dist1 = round(gibbs_dist1);
    d = sum(gibbs_dist1,1); d = d./sum(d); d(d==0) = 1;
    nTransformations = -d * log(d)';
    
    % correct generalization capacity
    gc{k} = gc{k}-log(k)+nTransformations;
    
    %%
    
    % transforming units from nats to bits
    gc{k} = gc{k} * log2(exp(1));
    %% Information content (ASC)
    % Quality of algorithm: 
    % $\mathcal{I} := \max_{\beta} GC(\beta)$
    [info_content(k),max_gc_idx(k)] = max(gc{k}); 
    
    %% Bayesian Information Criterion (BIC)
    % $BIC := m \times k \times ln (n) + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the 
    % number of bins and $c^{\perp}$ is the empirical risk minimizer
    
    if ~smooth_clustering
        cost = sum(sum(gibbs_dist1 .* (-connectivity_matrix{1} * log(centroids1)')));
    else
        cost = sum(sum(potential1(logical(gibbs_dist1))));
    end
    BIC(k) = size(connectivity_matrix{1},2) * k * log(size(connectivity_matrix{1},1)) + 2 * cost;
    
    %% Akaike Information Criterion (AIC)
    % $AIC := 2 \times m \times k + 2 R(\mathbf{X},\hat{\theta})$ where $m$ is the 
    % number of bins and $c^{\perp}$ is the empirical risk minimizer
    
    AIC(k) = 2 * size(connectivity_matrix{1},2) * k + 2 * cost;
    
    %% Bayesian evidence
    % $p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \phi)
    % p(\theta \mid \alpha) d\phi = \prod_k \frac{B(\alpha_k +
    % \mathbf{n}_k)}{B(\alpha_k)}$ where $\mathbf{n}_k := (n_{k1},n_{k2},\ldots,n_{kd})$
    % and $n_{kj} := \sum_{i:c(i)=k} x_{ij}$
    
    [~,labels] = max(gibbs_dist1,[],2);
    log_bayes_evidence{1}(k) = 0;
    log_bayes_evidence{2}(k) = 0;
    for c = unique(labels)'
        ncj = full(sum(connectivity_matrix{1}(labels==c,:),1));
        alpha_tmp{1} = alpha{1}/k; alpha_tmp{1}(ncj==0) = [];
        alpha_tmp{2} = alpha{2}; alpha_tmp{2}(ncj==0) = [];
        ncj(ncj==0) = [];
        
        log_bayes_evidence{1}(k) = log_bayes_evidence{1}(k) + sum(gammaln(alpha_tmp{1} + ncj)) ...
            + gammaln(sum(alpha_tmp{1})) - gammaln(sum(alpha_tmp{1} + ncj)) - sum(gammaln(alpha_tmp{1}));
        log_bayes_evidence{2}(k) = log_bayes_evidence{2}(k) + sum(gammaln(alpha_tmp{2} + ncj)) ...
            + gammaln(sum(alpha_tmp{2})) - gammaln(sum(alpha_tmp{2} + ncj)) - sum(gammaln(alpha_tmp{2}));
    end
    %% Number of misclustered seed voxels
    
    number_misclustered_objects(k) = sum(diag(round(gibbs_dist1)' * ~round(gibbs_dist2(:,match_clusters_idx))))/2;
    
    %% Distance between centroids
    [~,max_gc_idx] = max(gc{k});
    centroids_opt = double(gibbs_dist_packed1{k}(:,:,max_gc_idx))'*connectivity_matrix{1};
    centroids_opt = bsxfun(@rdivide,centroids_opt,sum(centroids_opt,2));
    centroids_opt(centroids_opt==0) = eps;
    
    %% Results
    display_result(gc,info_content,gibbs_dist_packed1,gibbs_dist_packed2,inv_temp_pack,log_bayes_evidence,BIC,AIC,K(1:find(K==k)),...
        dsim,centroids_opt,number_misclustered_objects,seed_coords);
end

%% Runtime

% display runtime
disp(['runtime: ' num2str(toc/60) ' minutes on ' computer])

%%

% save resutls
%save([path.results_directory 'parcellation.mat'],'gibbs_dist_packed1','gibbs_dist_packed2','gc','info_content','BIC','AIC','log_bayes_evidence','dsim_across_instances')
save([path.results_directory 'parcellation.mat'],'gibbs_dist_packed1','gibbs_dist_packed2','gc','info_content','BIC','AIC','log_bayes_evidence')

##### SOURCE END #####
--></body></html>
